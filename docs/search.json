[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A course in Geographic Data Science",
    "section": "",
    "text": "Welcome\nThis is the website for the “Geographic Data Science” module ENVS363/563 at the University of Liverpool. This is course designed and delivered by Dr. Elisabetta Pietrostefani and Dr. Carmen Cabrera from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Much of the course material is inspired by Dani Arribas-Bel’s course on Geographic Data Science. The previous version of this course is available here.\nThis module will introduce students to the field of Geographic Data Science (GDS), a discipline established at the intersection between Geographic Information Science (GIS) and Data Science. The course covers how the modern GIS toolkit can be integrated with Data Science tools to solve practical real-world problems.\nCore to the set of employable skills to be taught in this course is an introduction to programming tools. Students will be able to whether to develop their skills in either R or Python in Lab sessions.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "A course in Geographic Data Science",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - Module Leader - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Part 1\nWeek 1 - Introduction and Open Science\nWeek 2 - Spatial Data\nWeek 3 - Mapping Vector Data\nWeek 4 - Mapping Raster Data\nWeek 5 - Points\nWeek 6 - No Lecture & Clinic\nAssignment I: Programmed Map (40% of Final Grade)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#part-2",
    "href": "syllabus.html#part-2",
    "title": "Syllabus",
    "section": "Part 2",
    "text": "Part 2\nWeek 7 - Spatial Weights\nWeek 8 - ESDA\nWeek 9 - Clustering\nWeek 10 - Spatial Network Analysis\nWeek 11 - No lecture & Clinic\nAssignment II: A computational essay (60% of Final Grade)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aims\nThe module has three main aims.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "",
    "text": "Provide students with core competences in Geographic Data Science (GDS). This includes advancing their statistical and numerical literacy and introducing basic principles of programming and state-of-the-art computational tools for GDS;\nPresent a comprehensive overview of the main methodologies available to the Geographic Data Scientist, as well as their intuition as to how and when they can be applied;\nFocus on real world applications of these techniques in a geographical and applied context.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\nFor all\n\nDemonstrate advanced GIS/GDS concepts and be able to use the tools programmatically to import, manipulate and analyse data in different formats.\nUnderstand the motivation and inner workings of the main methodological approaches of GDS, both analytical and visual.\nEvaluate the suitability of a specific technique, what it can offer and how it can help answer questions of interest.\nApply a number of spatial analysis techniques and how to interpret the results, in the process of turning data into information.\nWhen faced with a new data-set, work independently using GIS/GDS tools programmatically.\n\nOnly for MSc students\n\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "Overview",
    "section": "Feedback",
    "text": "Feedback\nFormal assessment of one map and one computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assess.html",
    "href": "assess.html",
    "title": "Assessments",
    "section": "",
    "text": "Assignment I\nThis assignment will be evaluated on technical data processing, map design abilities (assemblage), and design overall narrative.\nOnce you have created your map, you will need to present it. Write up to 500 about the choices you made to create the map.\nYou will submit an .html file obtained by rendering your .qmd in R or .ipynb Jupyter Notebook in Python.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#assignment-i",
    "href": "assess.html#assignment-i",
    "title": "Assessments",
    "section": "",
    "text": "Title: Programmed Map\nType: Coursework\nDue date: 30th October 2025 (week 6)\n40% of the final mark\nChance to be reassessed\nElectronic submission only\n\n\n\n\nFull Assignment details to come\n\n\n\nIf you are doing the assignment in R: You can start from this .qmd file to render the html. Other file formats will not be accepted.\nIf you are doing the assignment in Python: To do so, in your .ipynb file, follow these steps: File –&gt; Save and Export as.. –&gt; HTML. Prior to this step, the notebook needs to be rendered (i.e. all the cells should be executed). Other file formats will not be accepted.\n\n\nSubmit\nOnce completed, you will need to submit the following:\nAn html version of an .qmd document with R integrated code.\nThe assignment will be evaluated based on three main pillars, on which you will have to be successful to achieve a good mark:\n\nData Processing: Your proficiency in handling and manipulating data will be a fundamental aspect of the assessment.\nMap assemblage This includes your ability to master technologies that allow you to create a compelling map.\nDesign and narrative: Your success in designing an appealing map with a compelling narrative will play a pivotal role in your overall evaluation.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#assignment-ii",
    "href": "assess.html#assignment-ii",
    "title": "Assessments",
    "section": "Assignment II",
    "text": "Assignment II\n\nTitle: Computational Essay\nType: Coursework\nDue date: 4th December 2025 (week 11)\n60% of the final mark\nChance to be reassessed\nElectronic submission only\n\nA 4,000 word computational essay on a geographic data set which they have explored and analysed using the skills and techniques developed during the course. Students will complete an essay which combines both code, data visualisation and prose supported by references in order to demonstrate sound understanding of all learning outcomes.\n\nFull Assignment details to come\n\nImportant information about data access through the US Census API:\n\nIn R: ENVS2425-363-563.2.qmd\nIn Python: ENVS2425-363-563.2.ipynb\n\nOverview\nHere’s the premise. You will take the role of a real-world geographic data scientist tasked to explore datasets on Los Angeles and find useful insights for a variety of city decision-makers. It does not matter if you have never been to the Los Angeles. In fact, this will help you focus on what you can learn about the city through the data, without the influence of prior knowledge. Furthermore, the assessment will not be marked based on how much you know about Los Angeles but instead about how much you can show you have learned through analysing data. You will need contextualise your project by highlighting the opportunities and limitations of ‘old’ and ‘new’ forms of spatial data and reference relevant literature.\nWhat is a Computational Essay?\nA computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. This piece of assessment is equivalent to 4,000 word. However, this is the overall weight. Since you will need to create not only narrative but also code and figures, here are the requirements:\n\nMaximum of 1,000 words of ordinary text (references do not contribute to the word count). You should answer the specified questions within the narrative. The questions should be included within a wider analysis.\nUp to four maps or figures (a figure may include more than one map and will only count as one but needs to be integrated in the same overall output)\nUp to one table\n\nThere are three kinds of elements in a computational essay:\n\nOrdinary text (in English)\nComputer input (R or Python)\nComputer output\n\nThese three elements all work together to express what’s being communicated.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#marking-criteria",
    "href": "assess.html#marking-criteria",
    "title": "Assessments",
    "section": "Marking Criteria",
    "text": "Marking Criteria\nThis course follows the standard marking criteria (the general ones and those relating to GIS assignments in particular) set by the School of Environmental Sciences. Please make sure to check the student handbook and familiarise with them. In addition to these generic criteria, the following specific criteria will be used in cases where computer code is part of the work being assessed:\n\n0-15: the code does not run and there is no documentation to follow it.\n16-39: the code does not run, or runs but it does not produce the expected outcome. There is some documentation explaining its logic.\n40-49: the code runs and produces the expected output. There is some documentation explaining its logic.\n50-59: the code runs and produces the expected output. There is extensive documentation explaining its logic.\n60-69: the code runs and produces the expected output. There is extensive documentation, properly formatted, explaining its logic.\n70-79: all as above, plus the code design includes clear evidence of skills presented in advanced sections of the course (e.g. custom methods, list comprehensions, etc.).\n80-100: all as above, plus the code contains novel contributions that extend/improve the functionality the student was provided with (e.g. algorithm optimizations, novel methods to perform the task, etc.).",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#generative-artificial-intelligence",
    "href": "assess.html#generative-artificial-intelligence",
    "title": "Assessments",
    "section": "Generative Artificial Intelligence",
    "text": "Generative Artificial Intelligence\n• You are reminded that the inappropriate use of Generative Artificial Intelligence Tools in the preparation of assignments is strictly prohibited.\n• Assignments should be prepared using your own words. All use of AI translation tools should be properly acknowledged. Extensive use of AI proof-reading tools is prohibited. Whilst you may use spelling/grammar checks typically found in word-processing packages, using AI tools to change words/sentence structure may incur an Academic Integrity penalty.\n• If your assessment is referred for an Academic Integrity Investigation, you may be asked to demonstrate that the work you have submitted is your own. Therefore, it is advised that you keep hold of earlier files, drafts, notes and other relevant preparatory materials that you have used.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "environ.html",
    "href": "environ.html",
    "title": "Environment",
    "section": "",
    "text": "Coding Languages\nThis course can be followed by anyone with access to a bit of technical infrastructure. This section details the set of local and online requirements you will need to be able to follow along, as well as instructions or pointers to get set up on your own. This is a centralised section that lists everything you will require.\nIn this course, you have the option to follow along using either R or Python, depending on your past experience with these programming languages and preference. Please choose one language to focus on and stick to it throughout.\nThis course has two assignments and you will be required to submit both assignments in the same programming languages. The next two sections will guide you through the process of setting up your development environment in R or Python, so you can get started with the course smoothly.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environ.html#coding-languages",
    "href": "environ.html#coding-languages",
    "title": "Environment",
    "section": "",
    "text": "If you want to follow the course in R, you can find instructions to set up your environment here.\nIf you want to follow the course in Python, you can find instructions to set up your environment here.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environ.html#reproducing-code-in-this-book",
    "href": "environ.html#reproducing-code-in-this-book",
    "title": "Environment",
    "section": "Reproducing code in this book",
    "text": "Reproducing code in this book\nIf you want to reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environR.html",
    "href": "environR.html",
    "title": "R",
    "section": "",
    "text": "R Basics\nTo run the analysis and reproduce the code in R, you need the following software:\nTo install and update:\nTo check your version of:",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#r-basics",
    "href": "environR.html#r-basics",
    "title": "R",
    "section": "",
    "text": "Starting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio&gt;Preferences&gt;Appearance and RStudio&gt;Preferences&gt;Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; but you can chose specific themes.\n\n\n\n\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, you can check it with getwd()\n\n\n\n\n\n\n\n\n\nImportant: You do not need to set your working directory if you are using an R-markdown or Quarto document and you have it saved in the right location. The pathway will start from where your document is saved.\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name &lt;- value. Assignment can also be performed using = instead of &lt;-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator &lt;- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage &lt;- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing &lt;- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n&gt; greater than\n&gt;= greater or equal to\n&lt;= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#installing-packages",
    "href": "environR.html#installing-packages",
    "title": "R",
    "section": "Installing packages",
    "text": "Installing packages\nIn R, packages are collections of functions, compiled code and sample data. They functionally act as “extensions” to the base R language, and can help you accomplish all operations you might want to perform in R (if no package serves your purpose, you may want to write an entirely new one!). Now, we will install the R package tidyverse. Look at the link to see what tidyverse includes, and directly load a .csv file (comma-separated values) into R from your computer.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#examples",
    "href": "environR.html#examples",
    "title": "R",
    "section": "Examples",
    "text": "Examples\nLet’s create some random R objects:\n\n## Entering random \nLondon  &lt;- 8982000 # population\nBristol &lt;- 467099 # population\nLondon_area &lt;-1572 # area km2\nBristol_area &lt;-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens &lt;- London/London_area\nBristol_pop_dens &lt;- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density &lt;- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx &lt;- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#data-structures",
    "href": "environR.html#data-structures",
    "title": "R",
    "section": "Data Structures",
    "text": "Data Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny &lt;- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz &lt;- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities &lt;- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm &lt;- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn &lt;- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data &lt;- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x&lt;7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column &lt;- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11 0.12057209\n2   b  2 12 0.16461165\n3   c  3 13 0.10111758\n4   d  4 14 0.16387671\n5   e  5 15 0.57811083\n6   f  6 16 0.06151680\n7   g  7 17 0.70395518\n8   h  8 18 0.04066187\n9   i  9 19 0.33875033\n10  j 10 20 0.66096785",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#exercises",
    "href": "environR.html#exercises",
    "title": "R",
    "section": "Exercises",
    "text": "Exercises\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector &lt;- c(\"Elisabetta\", \"Carmen\", \"Habib\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform &lt;- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 2.5494715 1.3127550 2.4584536 0.6895074 2.2425527 1.3976348 0.7799749\n [8] 1.4578196 0.2121431 2.2033858\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector &lt;- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix &lt;-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df &lt;-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\n\nShow the code\nnew_df &lt;-  new_df %&gt;%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %&gt;%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#import-data-from-csv",
    "href": "environR.html#import-data-from-csv",
    "title": "R",
    "section": "Import data from csv",
    "text": "Import data from csv\n\nDensities_UK_cities &lt;- read_csv(\"data/London/Tables/Densities_UK_cities.csv\")\n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): city, pop\ndbl (1): n\nnum (2): area, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDensities_UK_cities\n\n# A tibble: 76 × 5\n       n city               pop        area density\n   &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Greater London     9,787,426 1738.    5630\n 2     2 Greater Manchester 2,553,379  630.    4051\n 3     3 West Midlands      2,440,986  599.    4076\n 4     4 West Yorkshire     1,777,934  488.    3645\n 5     5 Greater Glasgow    957,620    368.    3390\n 6     6 Liverpool          864,122    200.    4329\n 7     7 South Hampshire    855,569    192     4455\n 8     8 Tyneside           774,891    180.    4292\n 9     9 Nottingham         729,977    176.    4139\n10    10 Sheffield          685,368    168.    4092\n# ℹ 66 more rows\n\n\nYou can also view the data set with:\n\nglimpse(Densities_UK_cities)\n\nRows: 76\nColumns: 5\n$ n       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ city    &lt;chr&gt; \"Greater London\", \"Greater Manchester\", \"West Midlands\", \"West…\n$ pop     &lt;chr&gt; \"9,787,426\", \"2,553,379\", \"2,440,986\", \"1,777,934\", \"957,620\",…\n$ area    &lt;dbl&gt; 1737.9, 630.3, 598.9, 487.8, 368.5, 199.6, 192.0, 180.5, 176.4…\n$ density &lt;dbl&gt; 5630, 4051, 4076, 3645, 3390, 4329, 4455, 4292, 4139, 4092, 42…\n\ntable(Densities_UK_cities$city)\n\n\n               Aberdeen  Accrington/ Rossendale Barnsley/ Dearne Valley \n                      1                       1                       1 \n               Basildon             Basingstoke                 Bedford \n                      1                       1                       1 \n                Belfast              Birkenhead               Blackburn \n                      1                       1                       1 \n              Blackpool      Bournemouth/ Poole       Brighton and Hove \n                      1                       1                       1 \n                Bristol                 Burnley       Burton-upon-Trent \n                      1                       1                       1 \n              Cambridge                 Cardiff              Chelmsford \n                      1                       1                       1 \n             Cheltenham            Chesterfield              Colchester \n                      1                       1                       1 \n               Coventry                 Crawley                   Derby \n                      1                       1                       1 \n              Doncaster                  Dundee              Eastbourne \n                      1                       1                       1 \n              Edinburgh                  Exeter  Farnborough/ Aldershot \n                      1                       1                       1 \n             Gloucester         Greater Glasgow          Greater London \n                      1                       1                       1 \n     Greater Manchester                 Grimsby                Hastings \n                      1                       1                       1 \n           High Wycombe                 Ipswich      Kingston upon Hull \n                      1                       1                       1 \n              Leicester                 Lincoln               Liverpool \n                      1                       1                       1 \n                  Luton               Maidstone               Mansfield \n                      1                       1                       1 \n           Medway Towns           Milton Keynes              Motherwell \n                      1                       1                       1 \n                Newport             Northampton                 Norwich \n                      1                       1                       1 \n             Nottingham                  Oxford       Paignton/ Torquay \n                      1                       1                       1 \n           Peterborough                Plymouth                 Preston \n                      1                       1                       1 \n                Reading               Sheffield                  Slough \n                      1                       1                       1 \n        South Hampshire         Southend-on-Sea          Stoke-on-Trent \n                      1                       1                       1 \n             Sunderland                 Swansea                 Swindon \n                      1                       1                       1 \n               Teesside                 Telford                  Thanet \n                      1                       1                       1 \n               Tyneside              Warrington           West Midlands \n                      1                       1                       1 \n         West Yorkshire                   Wigan               Worcester \n                      1                       1                       1 \n                   York \n                      1",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#r-list-of-libraries",
    "href": "environR.html#r-list-of-libraries",
    "title": "R",
    "section": "R List of libraries",
    "text": "R List of libraries\nThe list of libraries used in this book is provided below:\n\ntidyverse\ndata.table\nsf\ntmap\nreadr\ngeojsonsf\nosmdata\nbasemapR\nRColorBrewer\nclassInt\nR.utils\ndplyr\nggplot2\nviridis\nraster\nterra\nexactextractr\ntidyterra\nspdep\ntibble\npatchwork\nrosm\ntidyr\nGGally\ncluster\nrgeoda\nmapview\nggspatial\ncolorspace\ngstat\nspatstat\ndbscan\nfpc\neks\nigraph\ntidygraph\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\n# package names\npackages &lt;- c(\n  \"tmap\", \"readr\", \"geojsonsf\", \"osmdata\", \"basemapR\", \"sf\", \"tidyverse\", \n  \"RColorBrewer\", \"classInt\", \"R.utils\", \"dplyr\", \"ggplot2\", \"viridis\", \n  \"raster\", \"terra\", \"exactextractr\", \"tidyterra\", \"spdep\", \"tibble\", \n  \"patchwork\", \"rosm\", \"tidyr\", \"GGally\", \"cluster\", \"rgeoda\", \"mapview\", \n  \"ggspatial\", \"colorspace\", \"gstat\", \"spatstat\", \"dbscan\", \"fpc\", \"eks\", \n  \"igraph\", \"tidygraph\"\n)\n\n# Function to check and install missing packages\ninstall_if_missing &lt;- function(p) {\n  if (!requireNamespace(p, quietly = TRUE)) {\n    install.packages(p)\n  }\n}\n\n# Apply the function to each package\ninvisible(sapply(packages, install_if_missing))",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#resources",
    "href": "environR.html#resources",
    "title": "R",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nR for Data Science. R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.\nSpatial Data Science by Edzer Pebesma and Roger Bivand introduces and explains the concepts underlying spatial data.\nGeo-computation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow.",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environPy.html",
    "href": "environPy.html",
    "title": "Python",
    "section": "",
    "text": "Set up Miniconda (and Python) on Ms Windows\nTo run the analysis and reproduce the code in Python, you will need to set up the Python environment to:\nWe will use Miniconda to handle our working environment.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#set-up-miniconda-and-python-on-ms-windows",
    "href": "environPy.html#set-up-miniconda-and-python-on-ms-windows",
    "title": "Python",
    "section": "",
    "text": "Installation\n\nInstall Miniconda:\n\nOption 1: On a UoL Machine: Anaconda is installed on many university machines. Please check whether it is installed. If not, download and install Anaconda through from Install University Applications, type and choose Anaconda.\nOption 2: Recommended - Install Miniconda on your personal Laptop: Follow the instructions here.\n\nDuring the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\n\n\n\n\n\nUniversity Machines\n\n\n\nIf you do choose to work on University Machines\n\nChose a machine where Anaconda has been pre-installed.\nAlways use the same machine. For example if on the first day you are using CT60 Station 17 - Orange Zone, continue using this machine for the rest of the course. If you change machine you will need to re-install the environment every time.\n\n\n\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it envs363_563. You can save it wherever you want. If you are working on a university machine, it could be worth creating it in M:/, which is your “virtual” hard-disk.\n\n\n\nDownload the data to run and render the jupyter notebooks. To learn how to download folders from github see here.\nUnzip the folders and move the nested folders into the folder envs363_563.\nCreate another folder called labs\n\nThe folder structure should look like:\nenvs363_563/\n├── data/\n└── labs/\n\n\nSet up the Python Environment\n\nDownload the envs363_563.yml from GitHub by cliciking Download raw file, top right at this page\nSave it in the folder envs363_563 created before.\nType in the search bar and find the Anaconda Powershell Prompt if working on University Machine or Anaconda Prompt (miniconda 3) if on your personal. Launch it. The terminal should appear.\n\n\n\nIn the Anaconda Terminal write: conda env create -n envs363_563 --file M:\\envs363\\envs363_563.yml and press Enter; if the file is located elsewhere you’ll need to use the corresponding file path.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nIn the Anaconda Terminal write conda activate envs363_563 and press Enter. This activates your working environment.\n\n\n\n\nNecessary on University machines, otherwise Optional: Configuration of Jupyter Notebooks\n\nIn the Anaconda Terminal, write jupyter server --generate-config and press enter. This, at least in Windows, should create a file to: C:\\Users\\username\\.jupyter\\jupyter_server_config.py.\nOpen the file with a text editor (e.g. Notepad++), do a ctrl-f search for: c.ServerApp.root_dir, uncomment it by removing the # and change it to c.ServerApp.notebook_dir = 'M:\\\\your\\\\new\\\\path, for example the directory where you created the envs363_563 folder. In the University Machines, it is advised to work on the directory M:\\.\nSave the file and close it.\n\n\n\n\n\nStart a Lab Session\n\nDownload the Jupyter Notebook of the session in your folder. Choose one jupyter notebook and click Dowload raw file as shown below.\n\n\n\nSave the file in the labs folder within your envs363_563 folder on your machine.\nType in the search bar, find and open the Anaconda Prompt (miniconda 3).\nIn the Anaconda Terminal write and run conda activate envs363_563.\nIn the Anaconda Terminal write and run jupyter notebook. This should open Jupyter Notebook in your default browser.\n\n\n\nNavigate to your course folder in and double click on the notebook downloaded in step 1.\nYou can now work on your copy of the notebook.\n\nFollow these instructions and test your installation prior to the first Lab Session. If you experience any issues, write a message on the Ms Teams channel of the module. Setting up the Python environment is necessary for:\n\nExecuting the Jupyter Notebooks of the Lab sessions of the course.\nPreparing your own Jupyter Notebooks for the assignments (one each).",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#set-up-miniconda-and-python-on-mac",
    "href": "environPy.html#set-up-miniconda-and-python-on-mac",
    "title": "Python",
    "section": "Set up Miniconda (and Python) on MAC",
    "text": "Set up Miniconda (and Python) on MAC\n\nInstallation\nTo install Miniconda on your personal laptop, Follow the instructions here. During the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it envs363_563. You can save it wherever you want. For example, Elisabetta has named her folder envs363_563 and it’s in her Dropbox in Users/PIETROST/Library/CloudStorage/Dropbox/envs363_563\nDownload the data to run and render the jupyter notebooks. To learn how to download folders from github see here.\nUnzip the folders and move the nested folders into the folder envs363_563.\nCreate another folder called labs\n\nThe folder structure should look like:\nenvs363_563/ ├── data/ └── labs/\n\n\n\nSet up the Python Environment\n\nDownload the envs363_563.yml from GitHub by clicking Download raw file, top right at this page\nSave it in the folder envs363_563 created before.\nType in the search bar and open the Terminal.\nIn the Terminal write conda env create -n envs363 --file envs363_563.yml and press Enter. This will need to be modified according to where you placed the envs363_563 folder. For example, Elisabetta has named her folder envs363_563 and it’s in her Dropbox in Users/PIETROST/Library/CloudStorage/Dropbox/envs363_563/envs363_563.yml. If you created the envs363_563 folder on your desktop, the path would be Desktop/envs363_563.\n\n\n\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nYou should then see this\n\n\n\n\nStart a Lab Session\n\nDownload the Jupyter Notebook of the session in your folder. Choose one jupyter notebook and click Dowload raw file as shown below\n\n\n\nSave the file in the labs folder within your envs363 folder on your machine.\nType in the search bar, find and open the Terminal.\nIn the Terminal write and run conda activate envs363.\nIn the Terminal write and run jupyter notebook.\n\n\n\nThis should open Jupyter Notebook in your default browser. You should see something like this:\n\n\n\nNavigate to your folder. You can now work on your copy of the notebook.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#py-basics",
    "href": "environPy.html#py-basics",
    "title": "Python",
    "section": "Py Basics",
    "text": "Py Basics\nPlease refer to the tutorials from learnpython.org for an introduction to coding in Python. We particularly recommend the tutorials listed under the “Learn the Basics” section.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#resources",
    "href": "environPy.html#resources",
    "title": "Python",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python.\nPython for Geographic Data Analysis",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download data from github",
    "section": "",
    "text": "Go to https://download-directory.github.io\n\n\n\nGo to the folder you need for your Lab. For example copy: https://github.com/pietrostefani/gds/tree/main/data/London\n\n\n\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "Environment",
      "Download data from github"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "From Geographic Information Systems to Geographic Data Science\nIn this section we introduce what Geographic Data Science is. We top it up with a few (optional) further readings for the interested and curious mind.\nSlides can be downloaded “here”\nGeographic Information holds a pivotal position within our modern societies, permeating various aspects of our daily lives. It underpins essential sectors such as housing, transportation, insurance, banking, telecommunications, logistics, energy, retail, agriculture, healthcare, and urban planning. Its significance lies in the capacity to analyze and derive invaluable insights from geo-spatial data, enabling us to make informed decisions and address complex challenges. Proficiency in this field equips individuals with the ability to work with real-world data across multiple domains and tackle diverse problems. Furthermore, it provides the opportunity to acquire essential data science skills and utilize important tools for answering spatial questions. Given its wide-ranging applications and the increasing reliance on location-based information, there is a substantial demand for experts in the geographic information industry, making it a highly sought-after skill set in today’s workforce.\nWhat information does GIS use?\nFrom the real-world to the GIS world\nGeographic Data Science\nA GIS person typically produces cartographic and analytical products using desktop software. A geospatial data scientist creates code and runs pipelines that produce analytical products and cartographic representations.\nThis entails working with real-world data from various domains and tackling a wide range of complex problems. Through this process geospatial data science includes both data science and GIS tools that lead to the analysos of intricate spatial questions effectively. The synergy between CyberGIS and Geographic Data Science is unmistakable, with coding playing a pivotal role in enabling the seamless development of interactive data analysis. By leveraging cutting-edge technologies and innovative methodologies, this symbiotic relationship enhances the accessibility, scalability, and interactivity of geospatial data analysis. Consequently, it opens up new vistas for collaborative research and decision-making processes.\nThis multifaceted approach equips them with the knowledge and expertise to navigate the intricate world of spatial data analysis and contribute meaningfully to diverse fields where location-based insights are invaluable.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#from-geographic-information-systems-to-geographic-data-science",
    "href": "intro.html#from-geographic-information-systems-to-geographic-data-science",
    "title": "1 Introduction",
    "section": "",
    "text": "Data that defines geographical features like roads, rivers\nSoil types, land use, elevation\nDemographics, socioeconomic attributes\nEnvironmental, climate, air-quality\nAnnotations that label features and places",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-is-geographic-data-science",
    "href": "intro.html#what-is-geographic-data-science",
    "title": "1 Introduction",
    "section": "What is Geographic Data Science?",
    "text": "What is Geographic Data Science?\nStatistician George Box :\nAll models are wrong, but some are useful In a similar fashion.\nGeographer Keith Ord :\nAll maps are wrong, but some are useful.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#open-source-gis",
    "href": "intro.html#open-source-gis",
    "title": "1 Introduction",
    "section": "Open Source GIS",
    "text": "Open Source GIS\nOpen source Geographic Information Systems (GIS), such as QGIS, have made geographic analysis accessible worldwide. GIS programs tend to emphasize graphical user interfaces (GUIs), with the unintended consequence of discouraging reproducibility (although many can be used from the command line Python + QGIS). R and Python by contrast, emphasizes the command line interface (CLI).\nThe ‘geodata revolution’ drives demand for high performance computer hardware and efficient, scalable software to handle and extract signal from the noise, to understand and perhaps change the world. Spatial databases enable storage and generation of manageable subsets from the vast geographic data stores, making interfaces for gaining knowledge from them vital tools for the future.\nR and Python are both tools with advanced modeling and visualization capabilities.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#open-science",
    "href": "intro.html#open-science",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#modern-scientific-tools",
    "href": "intro.html#modern-scientific-tools",
    "title": "1 Introduction",
    "section": "Modern Scientific Tools",
    "text": "Modern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let’s dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\n\nSecond half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "openscienceR.html",
    "href": "openscienceR.html",
    "title": "OpenScience in R",
    "section": "",
    "text": "Data wrangling\nNow that you know what computational notebooks are and why we should care about them, let’s start using them! This section introduces you to using R for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running the code on your own.\nOnce you have read through, jump on the Do-It-Yourself section, which will provide you with a challenge that you should complete on your own, and will allow you to put what you have already learnt into practice.\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in R so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#loading-packages",
    "href": "openscienceR.html#loading-packages",
    "title": "OpenScience in R",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nlibrary(tidyverse) # a structure of data manipulation including several packages \nlibrary(data.table) # load the data.table package",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#datasets",
    "href": "openscienceR.html#datasets",
    "title": "OpenScience in R",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 &lt;- read.csv(\"data/census2021_ethn/liv_pop.csv\", row.names = \"GeographyCode\")\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read.csv from base R, you could also use read_csv from library(\"readr\").\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read.csv.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the census2021_ethn file by right-clicking on this link and saving the file\nPlace the file in a data folder you have created where you intend to read it.\nYour folder should have the following structure a. a gds folder (where you will save your quarto .qmd documents) b. a data folder c. the census2021_ethn folder inside your data folder.\n\n\n\n\n\n\n\n\n\nDownload a folder on github\n\n\n\n\nFirst go to https://download-directory.github.io/\nThen go to the folder you need to today. So for example copy: https://github.com/pietrostefani/gds/tree/main/data/London\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#data-sliced-and-diced",
    "href": "openscienceR.html#data-sliced-and-diced",
    "title": "OpenScience in R",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\nview(census2021)\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, tidyverse in R will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nhead(census2021) # read first 5 rows\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania\nE01006512                      0\nE01006513                      7\nE01006514                      5\nE01006515                      2\nE01006518                      4\nE01006519                      3\n\ntail(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033763   1302     68                  142                             11\nE01033764   2106     32                   49                             15\nE01033765   1277     21                   33                             17\nE01033766   1028     12                   20                              8\nE01033767   1003     29                   29                              5\nE01033768   1016     69                  111                             21\n          Antarctica.and.Oceania\nE01033763                      4\nE01033764                      0\nE01033765                      3\nE01033766                      7\nE01033767                      1\nE01033768                      6\n\n\nSummarise\nWe can get an overview of the values of the table:\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#columns",
    "href": "openscienceR.html#columns",
    "title": "OpenScience in R",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nUsing the package dplyr. More info can be found here. dplyr is a part of the tidyverse!\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(Total_Pop = rowSums(select(., Africa, Middle.East.and.Asia, Europe, The.Americas.and.the.Caribbean, Antarctica.and.Oceania)))\n\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Pop\nE01006512                      0      1880\nE01006513                      7      2941\nE01006514                      5      2108\nE01006515                      2      1208\nE01006518                      4      1696\nE01006519                      3      1286\n\n\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\ncensus2021$new_column &lt;- 1\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Pop new_column\nE01006512                      0      1880          1\nE01006513                      7      2941          1\nE01006514                      5      2108          1\nE01006515                      2      1208          1\nE01006518                      4      1696          1\nE01006519                      3      1286          1\n\n\ndplyr is an immensely useful package in R because it streamlines and simplifies the process of data manipulation and transformation. With its intuitive and consistent syntax, dplyr provides a set of powerful and efficient functions that make tasks like filtering, summarizing, grouping, and joining datasets much more straightforward. Whether you’re working with small or large datasets, dplyr’s optimized code execution ensures fast and efficient operations. Its ability to chain functions together using the pipe operator (%&gt;%) allows for a clean and readable code structure, enhancing code reproducibility and collaboration. Overall, dplyr is an indispensable tool for data analysts and scientists working in R, enabling them to focus on their data insights rather than wrestling with complex data manipulation code.\nDelete columns\nPermanently deleting variables is also within reach of one command:\ndplyr\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(new_column = 1)\n\n\ncensus2021 &lt;- census2021 %&gt;%\n  select(-new_column)",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#queries",
    "href": "openscienceR.html#queries",
    "title": "OpenScience in R",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe can select with c(). If this structure is new to you have a look here.\n\neu_tot_first4 &lt;- census2021 %&gt;%\n  filter(rownames(census2021) %in% c('E01006512', 'E01006513', 'E01006514', 'E01006515')) %&gt;%\n  select(Total_Pop, Europe)\n\neu_tot_first4\n\n          Total_Pop Europe\nE01006512      1880    910\nE01006513      2941   2225\nE01006514      2108   1786\nE01006515      1208    974\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 &lt;- census2021 %&gt;%\n  filter(Total_Pop &gt; 900)\n\nAreas where there are no more than 750 Europeans:\n\neuro750 &lt;- census2021 %&gt;%\n  filter(Europe &lt; 750)\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA &lt;- census2021 %&gt;%\n  filter(`Antarctica.and.Oceania` == 10)\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThese queries can grow in sophistication with almost no limits.\n\n\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 &lt;- census2021 %&gt;%\n  filter(The.Americas.and.the.Caribbean &gt; 25, Total_Pop &lt; 1500)\nac25_l500\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033750   1235     53                  129                             26\nE01033752   1024     19                  114                             33\nE01033754   1262     37                  112                             32\nE01033756    886     31                  221                             42\nE01033757    731     39                  223                             29\nE01033761   1138     52                  138                             33\n          Antarctica.and.Oceania Total_Pop\nE01033750                      5      1448\nE01033752                      6      1196\nE01033754                      9      1452\nE01033756                      5      1185\nE01033757                      3      1025\nE01033761                     11      1372",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#sorting",
    "href": "openscienceR.html#sorting",
    "title": "OpenScience in R",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted &lt;- census2021 %&gt;%\n  arrange(desc(Total_Pop)) #sorts the dataframe by the \"Total_Pop\" column in descending order \n\nhead(db_pop_sorted)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006747   2551    163                  812                             24\nE01006513   2225     61                  595                             53\nE01006751   1843    139                  568                             21\nE01006524   2235     36                  125                             24\nE01006787   2187     53                   75                             13\nE01006537   2180     23                   46                              6\n          Antarctica.and.Oceania Total_Pop\nE01006747                      2      3552\nE01006513                      7      2941\nE01006751                      1      2572\nE01006524                     11      2431\nE01006787                      2      2330\nE01006537                      2      2257",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#additional-resources",
    "href": "openscienceR.html#additional-resources",
    "title": "OpenScience in R",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "opensciencePy.html",
    "href": "opensciencePy.html",
    "title": "OpenScience in Python",
    "section": "",
    "text": "Data wrangling\nNow that you know what computational notebooks are and why we should care about them, let’s start using them! This section introduces you to using Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running the code on your own.\nOnce you have read through, jump on the Do-It-Yourself section, which will provide you with a challenge that you should complete on your own, and will allow you to put what you have already learnt into practice.\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#loading-packages",
    "href": "opensciencePy.html#loading-packages",
    "title": "OpenScience in Python",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\n# This ensures visualizations are plotted inside the notebook\n\nimport os              # This provides several system utilities\nimport pandas as pd    # This is the workhorse of data munging in Python",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#datasets",
    "href": "opensciencePy.html#datasets",
    "title": "OpenScience in Python",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 = pd.read_csv(\"data/census2021_ethn/liv_pop.csv\", index_col='GeographyCode')\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read_csv from the pandas library, which we have imported with the alias pd.\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument index_col is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read_csv because the file we want to read is in the csv format. However, pandas allows for many more formats to be read and write. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read_csv.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the census2021_ethn file by right-clicking on this link and saving the file\nPlace the file in a data folder you have created where you intend to read it.\nYour folder should have the following structure a. a gds folder (where you will save your quarto .qmd documents) b. a data folder c. the census2021_ethn folder inside your data folder.\n\n\n\n\n\n\n\n\n\nDownload a folder on github\n\n\n\n\nFirst go to https://download-directory.github.io/\nThen go to the folder you need to today. So for example copy: https://github.com/pietrostefani/gds/tree/main/data/London\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#data-sliced-and-diced",
    "href": "opensciencePy.html#data-sliced-and-diced",
    "title": "OpenScience in Python",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\ncensus2021\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n\n\nE01006513\n2225\n61\n595\n53\n7\n\n\nE01006514\n1786\n63\n193\n61\n5\n\n\nE01006515\n974\n29\n185\n18\n2\n\n\nE01006518\n1531\n69\n73\n19\n4\n\n\n...\n...\n...\n...\n...\n...\n\n\nE01033764\n2106\n32\n49\n15\n0\n\n\nE01033765\n1277\n21\n33\n17\n3\n\n\nE01033766\n1028\n12\n20\n8\n7\n\n\nE01033767\n1003\n29\n29\n5\n1\n\n\nE01033768\n1016\n69\n111\n21\n6\n\n\n\n\n298 rows × 5 columns\n\n\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, pandas will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\ncensus2021.head() # read first 5 rows\ncensus2021.tail() # read last 5 rows\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\nE01033764\n2106\n32\n49\n15\n0\n\n\nE01033765\n1277\n21\n33\n17\n3\n\n\nE01033766\n1028\n12\n20\n8\n7\n\n\nE01033767\n1003\n29\n29\n5\n1\n\n\nE01033768\n1016\n69\n111\n21\n6\n\n\n\n\n\n\n\nSummarise\nWe can get an overview of the values of the table:\n\ncensus2021.describe()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\n\n\ncount\n298.00000\n298.000000\n298.000000\n298.000000\n298.000000\n\n\nmean\n1462.38255\n29.818792\n62.909396\n8.087248\n1.949664\n\n\nstd\n248.67329\n51.606065\n102.519614\n9.397638\n2.168216\n\n\nmin\n731.00000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n1331.25000\n7.000000\n16.000000\n2.000000\n0.000000\n\n\n50%\n1446.00000\n14.000000\n33.500000\n5.000000\n1.000000\n\n\n75%\n1579.75000\n30.000000\n62.750000\n10.000000\n3.000000\n\n\nmax\n2551.00000\n484.000000\n840.000000\n61.000000\n11.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\ncensus2021.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nEurope\n298.0\n1462.382550\n248.673290\n731.0\n1331.25\n1446.0\n1579.75\n2551.0\n\n\nAfrica\n298.0\n29.818792\n51.606065\n0.0\n7.00\n14.0\n30.00\n484.0\n\n\nMiddle East and Asia\n298.0\n62.909396\n102.519614\n1.0\n16.00\n33.5\n62.75\n840.0\n\n\nThe Americas and the Caribbean\n298.0\n8.087248\n9.397638\n0.0\n2.00\n5.0\n10.00\n61.0\n\n\nAntarctica and Oceania\n298.0\n1.949664\n2.168216\n0.0\n0.00\n1.0\n3.00\n11.0",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#columns",
    "href": "opensciencePy.html#columns",
    "title": "OpenScience in Python",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nLonger, hardcoded:\n\ntotal = census2021['Europe'] + census2021['Africa'] + census2021['Middle East and Asia'] + census2021['The Americas and the Caribbean'] + census2021['Antarctica and Oceania']\n# Print the top of the variable\ntotal.head()\n\nGeographyCode\nE01006512    1880\nE01006513    2941\nE01006514    2108\nE01006515    1208\nE01006518    1696\ndtype: int64\n\n\nOne shot:\n\ncensus2021['Total_Population'] = census2021.sum(axis=1)\n# Print the top of the variable\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n\n\n\n\n\n\n\nNote that we are summing over “axis=1”. In a DataFrame object, “axis 0” and “axis 1” represent the rows and columns respectively.\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\n# New variable with all ones\ncensus2021['ones'] = 1\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\nones\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n1\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n1\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n1\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n1\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n1\n\n\n\n\n\n\n\nDelete columns\nPermanently deleting variables is also within reach of one command:\n\ndel census2021['ones']\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#queries",
    "href": "opensciencePy.html#queries",
    "title": "OpenScience in Python",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe use loc with lists:\n\neu_tot_first4 = census2021.loc[['E01006512', 'E01006513', 'E01006514', 'E01006515'], ['Total_Population', 'Europe']]\n\neu_tot_first4\n\n\n\n\n\n\n\n\nTotal_Population\nEurope\n\n\nGeographyCode\n\n\n\n\n\n\nE01006512\n1880\n910\n\n\nE01006513\n2941\n2225\n\n\nE01006514\n2108\n1786\n\n\nE01006515\n1208\n974\n\n\n\n\n\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 = census2021.loc[census2021['Total_Population'] &gt; 900, :]\npop900\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nE01033764\n2106\n32\n49\n15\n0\n2202\n\n\nE01033765\n1277\n21\n33\n17\n3\n1351\n\n\nE01033766\n1028\n12\n20\n8\n7\n1075\n\n\nE01033767\n1003\n29\n29\n5\n1\n1067\n\n\nE01033768\n1016\n69\n111\n21\n6\n1223\n\n\n\n\n298 rows × 6 columns\n\n\n\nAreas where there are no more than 750 Europeans:\n\neuro750 = census2021.loc[census2021['Europe'] &lt; 750, :]\neuro750\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01033757\n731\n39\n223\n29\n3\n1025\n\n\n\n\n\n\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA = census2021.loc[census2021['Antarctica and Oceania'] == 10, :]\noneOA\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006679\n1353\n484\n354\n31\n10\n2232\n\n\n\n\n\n\n\nPro-tip: These queries can grow in sophistication with almost no limits.\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 = census2021.loc[(census2021['The Americas and the Caribbean'] &gt; 25) &                    (census2021['Total_Population'] &lt; 1500), :]\nac25_l500\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01033750\n1235\n53\n129\n26\n5\n1448\n\n\nE01033752\n1024\n19\n114\n33\n6\n1196\n\n\nE01033754\n1262\n37\n112\n32\n9\n1452\n\n\nE01033756\n886\n31\n221\n42\n5\n1185\n\n\nE01033757\n731\n39\n223\n29\n3\n1025\n\n\nE01033761\n1138\n52\n138\n33\n11\n1372",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#sorting",
    "href": "opensciencePy.html#sorting",
    "title": "OpenScience in Python",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted = census2021.sort_values('Total_Population', ascending=False)\ndb_pop_sorted.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006747\n2551\n163\n812\n24\n2\n3552\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006751\n1843\n139\n568\n21\n1\n2572\n\n\nE01006524\n2235\n36\n125\n24\n11\n2431\n\n\nE01006787\n2187\n53\n75\n13\n2\n2330",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#additional-resources",
    "href": "opensciencePy.html#additional-resources",
    "title": "OpenScience in Python",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "spatialdata.html",
    "href": "spatialdata.html",
    "title": "2 Spatial Data",
    "section": "",
    "text": "Special about spatial data\nThis blocks explore spatial data, old and new. We start with an overview of traditional datasets, discussing their benefits and challenges for social scientists; then we move on to new forms of data, and how they pose different challenges, but also exciting opportunities. These two areas are covered with clips and slides that can be complemented with readings. Once conceptual areas are covered, we jump into working with spatial data in R or Python, which will prepare you for your own adventure in exploring spatial data.\nSlides can be downloaded “here”\nAll data is spatial - data comes from observation, and observation needs to happen somewhere and at some time. This makes all data spatial. For a lot of data, the location expressed in spatial, earth-bound coordinates of observation is not of prime importance:",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#special-about-spatial-data",
    "href": "spatialdata.html#special-about-spatial-data",
    "title": "2 Spatial Data",
    "section": "",
    "text": "If a patient undergoes a brain scan, the location of the scanner is not important; the location of the person relative to the scanner is.\nIf a person receives a positive COVID-19 test result, the location of testing may not be important for the person’s decision on whether to go into quarantine or not\nFor someone trying to do contact tracing, this person’s location history may be very relevant.",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#geometries",
    "href": "spatialdata.html#geometries",
    "title": "2 Spatial Data",
    "section": "Geometries",
    "text": "Geometries\nThese core spatial geometries are all supported in R package sf and Python library geopandas.\n\npoints\nlines\npolygons\nand their respective “multi” versions (which group entities of the same type into a single entity).\n\n\nGeometries\nThe basis of every type of geometry is the point. A point is simply a coordinate in 2D, 3D or 4D space such as:\n\n\nPOINT (5 2)\n\n A line string is a sequence of points with a straight line connecting the points, for example:\n\nLINESTRING (1 5, 4 4, 4 1, 2 2, 3 2)\n\nA polygon is a sequence of points that form a closed ring without intersection. Closed means that the first and the last point of a polygon have the same coordinates:\n\nPOLYGON ((1 5, 2 2, 4 1, 4 4, 1 5))\n\nA polygone with a hole: POLYGON ((1 5, 2 2, 4 1, 4 4, 1 5), (2 4, 3 4, 3 3, 2 3, 2 4)) \nAll types of objects:\n\n\n\n\n\nIllustration of point, linestring and polygon geometries.\n\n\n\n\nThere are also:\n\nsets of polygons, MULTIPOLYGON(((0 0,1 0,1 1,0 0)), ((3 3,4 3,4 4,3 3)))\ncombinations of these GEOMETRYCOLLECTION(POINT(0 1),LINESTRING(0 0,1 1))- which are inconvenient",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#good-old-geo-data",
    "href": "spatialdata.html#good-old-geo-data",
    "title": "2 Spatial Data",
    "section": "“Good old” (geo) data",
    "text": "“Good old” (geo) data\nTo understand what is new in new forms of data, it is useful to begin by considering traditional data. Datasets utilized in the field of social sciences exhibit several key characteristics:\n\nPurposeful Collection: These datasets are meticulously designed and gathered with specific research objectives in mind.\nRich Information: They provide a wealth of detailed and informative data, often offering a comprehensive “rich profile and portraits of the country” under examination.\nHigh Quality: Maintaining a high standard of data accuracy and integrity is a top priority in social science datasets.\n\nHowever, it’s important to note that these datasets also come with certain limitations:\n\nScale and Cost: Building and maintaining such datasets can be massive undertakings, often requiring substantial financial resources.\nCoarse Resolution: To safeguard privacy, data may need to be aggregated, resulting in a loss of fine-grained detail.\nSlowness: The process of data collection, curation, and dissemination can be time-consuming, leading to delays in availability.\nFrequency vs. Detail: Typically, as datasets become more detailed, their availability may decrease, making it challenging to access highly specific data on a regular basis.",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#new-forms-of-geo-data",
    "href": "spatialdata.html#new-forms-of-geo-data",
    "title": "2 Spatial Data",
    "section": "New forms of (geo) data",
    "text": "New forms of (geo) data\nNew forms of (geo) data are tied into the geo-data revolution. Data is often accidental, which is initially generated for various purposes but becomes available for analysis as a side effect. This data is incredibly diverse, varying in resolution and quality, but holds the potential for much greater detail in both spatial and temporal dimensions.\nHave a look at the two following articles:\n\nData ex Machina: Introduction to Big Data by Lazer & Radford\nAccidental, open and everywhere by Arribas-Bel",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#all-maps-are-wrong",
    "href": "spatialdata.html#all-maps-are-wrong",
    "title": "2 Spatial Data",
    "section": "All maps are wrong",
    "text": "All maps are wrong\n\n\nIf you’re still not convinced, try have a mess around with this link.\n\n\nCoordinates\nWith coordinates, we usually think a numbered measured along a ruler, where the ruler might be an imaginary line: it has an offset (0), a unit (m), and a constant direction. For spatial data we could have two imaginary lines perpendicular to each other, and we call this Cartesian space. Distance between \\((x_1,y_1)\\) and \\((x_2,y_2)\\) in Cartesian space is computed by Euclidean distance: \\[\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\\]\nLeft: geocentric coordinates (Cartesian, three-dimensional, units metres); Right: spherical/ellipsoidal coordinates (angles, units degrees)\n\n\n\n\n\n\n\n\n\nEuclidean distances do not work for ellipsoidal coordinates: one degree longitude at the equator is about 111 km, at the poles it is 0 km.\n\n\nWhat does coordinate reference system mean?\nCRSs if disregarded can lead to massive problems. CRSs allow you to make the right assumptions without having to guess. They specify what coordinates mean.\n“Data are not just numbers, they are numbers with a context” (Cobb & Moore)\nCoordinate reference systems provide the context of coordinates:\n\nThey tell whether the coordinates are ellipsoidal (angles), or derived, projected (Cartesian) coordinates\nIn case they are projected, they detail the kind of projection used, so that the underlying ellipsoidal coordinates can be recovered\nIn any case, they point out which ellipsoidal model (datum) was used.\n\nKnowing this we can:\n\nConvert between projected and unprojected, or to another projection\nTransform from one datum to another\nCombine the coordinates with any other coordinates that have a coordinate reference system\n\n\n\nProjection and transformation\nEstablished CRSs captured by EPSG codes are well-suited for many applications. A long and growing list of projections has been developed. Here are a few examples applied to the world so you can see that maps do change quite a bit when different projections are applied.\nThe Mollweide projection.\n\n\nCode\nworld_mollweide = st_transform(world, crs = \"+proj=moll\")\nplot(world[\"continent\"])\n\n\n\n\n\n\n\n\n\nOn the other hand, when mapping the world, it is often desirable to have as little distortion as possible for all spatial properties (area, direction, distance). One of the most popular projections to achieve as little distortion as possible is the Winkel tripel projection\n\n\nCode\nworld_wintri = lwgeom::st_transform_proj(world, crs = \"+proj=wintri\")\nplot(world_wintri[\"continent\"])\n\n\n\n\n\n\n\n\n\nSpecific PROJ parameters can be modified in most CRS definitions - you will most likely never use this. The below code transforms the coordinates to the Lambert azimuthal equal-area projection centered on longitude and latitude of 0.\n\n\nCode\nworld_laea1 = st_transform(world, \n                           crs = \"+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\")\nplot(world_laea1[\"continent\"])",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdata.html#further-readings",
    "href": "spatialdata.html#further-readings",
    "title": "2 Spatial Data",
    "section": "Further readings",
    "text": "Further readings\nWatch: Nathan Yau’s Flowing Data\n\nThe Problem with our maps by Nick Routely\nGeocomputation with R\nSpatial Data Science",
    "crumbs": [
      "2 Spatial Data"
    ]
  },
  {
    "objectID": "spatialdataR.html",
    "href": "spatialdataR.html",
    "title": "Lab in R",
    "section": "",
    "text": "Installing packages\nIn this lab, we will learn how to load, manipulate and visualize spatial data. In some senses, spatial data are usually included simply as “one more column” in a table. However, spatial is special sometimes and there are few aspects in which geographic data differ from standard numerical tables. In this session, we will extend the skills developed in the previous one about non-spatial data, and combine them. In the process, we will discover that, although with some particularities, dealing with spatial data in R largely resembles dealing with non-spatial data.\nWe will start by loading core packages for working with spatial data. See detailed description of R.\n# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# Using data from OpenStreetMap (OSM)\nlibrary(osmdata)\n# Static maps\nlibrary(basemapR)\nTo install basemapR you will need to do run\nlibrary(devtools)\ninstall_github('Chrisjb/basemapR')",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#datasets",
    "href": "spatialdataR.html#datasets",
    "title": "Lab in R",
    "section": "Datasets",
    "text": "Datasets\nToday we are going to go to London. We will be playing around with different datasets loading them both locally and dynamically from the web. You can download data manually, keep a copy on your computer, and load them from there.\n\nCreating geographic data\nFirst we will use the following commands create geographic datasets from scratch representing coordinates of some famous locations in London. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\npoi_df = tribble(\n  ~name, ~lon, ~lat,\n  \"The British Museum\",        -0.1459604, 51.5045975,\n  \"Big Ben\",    -0.1272057, 51.5007325,\n  \"King's Cross\", -0.1319481, 51.5301701,\n  \"The Natural History Museum\",     -0.173734, 51.4938451\n)\npoi_sf = sf::st_as_sf(poi_df, coords = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n\n\nTypes of Data\nNow let’s look at the different types of geographical data starting with polygons. We will use a dataset that contains the boundaries of the districts of London. We can read it into an object named districts.\n\nPolygonsLinesPoints\n\n\nWe first import the district shapefile use read_sf, we then plot it to make sure we are seeing it ‘correctly’. We us $geometry to plot just the geometry, if we don’t include $geometry R will plot the first 9 columns and if the dataset is large this is not advisable.\n\ndistricts &lt;- read_sf(\"data/London/Polygons/districts.shp\")\n\nplot(districts$geometry) # Create a simple plot\n\n\n\n\n\n\n\n\n\n\nWe them import a file of roads in London and plot it.\n\na_roads &lt;- read_sf(\"data/London/Lines/a_roads.shp\")\n\n# If you needed to import a `geojson` this would be the function.\n#a_roads &lt;- geojson_sf(\"data/London/Lines/a_roads.geojson\")\n\nplot(a_roads$geometry)\n\n\n\n\n\n\n\n\n\n\nWe can also import point files. So far, we have imported shapefiles and geojsons, but we can also obtain data from urls like in the Open Science DIY session or from other sources like OpenStreetMap. Both R and Python have libraries that allow us to query OpenStreetMap.\n\nosm_q_sf &lt;- opq(\"Greater London, U.K.\") %&gt;% # searching only in Greater London\n    add_osm_feature(key = \"amenity\", value = \"restaurant\") %&gt;% #adding osm data that is tagged as a museum\n  osmdata_sf () # transforming to sf object\n\nThe structure of osmdata objects are clear from their default print method, illustrated using the museum example. We will use them shortly.\n\nosm_q_sf  \n\nObject of class 'osmdata' with:\n                 $bbox : 51.2867601,-0.5103751,51.6918741,0.3340155\n        $overpass_call : The call submitted to the overpass API\n                 $meta : metadata including timestamp and version numbers\n           $osm_points : 'sf' Simple Features Collection with 26127 points\n            $osm_lines : 'sf' Simple Features Collection with 10 linestrings\n         $osm_polygons : 'sf' Simple Features Collection with 3218 polygons\n       $osm_multilines : NULL\n    $osm_multipolygons : 'sf' Simple Features Collection with 8 multipolygons\n\n\nYou do not need to know at this point what happens behind the scenes when we run these lines but, if you are curious, we are making a query to OpenStreetMap (almost as if you typed “museums in London, UK” within Google Maps) and getting the response as a table of data, instead of as a website with an interactive map. Pretty cool, huh?\nNote: the code cell above requires internet connectivity. Important: Be careful, if you query too much data, your environment is likely to get stuck.\n\nrestaurants_points &lt;- osm_q_sf$osm_points\n\nplot(restaurants_points$geometry)",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#inspecting-spatial-data",
    "href": "spatialdataR.html#inspecting-spatial-data",
    "title": "Lab in R",
    "section": "Inspecting Spatial Data",
    "text": "Inspecting Spatial Data\n\nInspecting\nJust like a dataframe (see the OpenScience Lab), we can inspect the data (attributes table) within a spatial object. The most direct way to get from a file to a quick visualization of the data is by loading it and calling the plot command. Let’s start by inspecting the data like we did for non spatial dataframes.\nWe can see our data is very similar to a traditional, non-spatial dataFrame, but with an additional column called geometry.\n\nhead(districts) # the command \"head\" reads the first 5 rows of the data\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  &lt;chr&gt;     &lt;chr&gt;                                                  &lt;POLYGON [m]&gt;\n1 00AA      City of London       ((531028.5 181611.2, 531036.1 181611.5, 531074…\n2 00AB      Barking and Dagenham ((550817 184196, 550814 184189.1, 550799 18416…\n3 00AC      Barnet               ((526830.3 187535.5, 526830.3 187535.4, 526829…\n4 00AD      Bexley               ((552373.5 174606.9, 552372.9 174603.9, 552371…\n5 00AE      Brent                ((524661.7 184631, 524665.3 184626.4, 524667.9…\n6 00AF      Bromley              ((533852.2 170129, 533850.4 170128.5, 533844.9…\n\n\nWe can inspect the object in different ways :\n\ndistricts[1,] # read first row\n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  &lt;chr&gt;     &lt;chr&gt;                                                  &lt;POLYGON [m]&gt;\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\ndistricts[,1] # read first column\n\nSimple feature collection with 33 features and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 503568.1 ymin: 155850.8 xmax: 561957.4 ymax: 200933.9\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 33 × 2\n   DIST_CODE                                                            geometry\n   &lt;chr&gt;                                                           &lt;POLYGON [m]&gt;\n 1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 18…\n 2 00AB      ((550817 184196, 550814 184189.1, 550799 184162.6, 550797.2 184159…\n 3 00AC      ((526830.3 187535.5, 526830.3 187535.4, 526829 187534.7, 526825.3 …\n 4 00AD      ((552373.5 174606.9, 552372.9 174603.9, 552371 174595.3, 552367.9 …\n 5 00AE      ((524661.7 184631, 524665.3 184626.4, 524667.9 184623.1, 524673.5 …\n 6 00AF      ((533852.2 170129, 533850.4 170128.5, 533844.9 170127.9, 533842.6 …\n 7 00AG      ((531410.7 181576.1, 531409.4 181573.1, 531409.4 181573.1, 531405 …\n 8 00AH      ((532745.1 157404.6, 532756.3 157394.6, 532768.1 157384, 532777.3 …\n 9 00AJ      ((512740.6 182181.6, 512740.1 182185.9, 512739.5 182190.5, 512739.…\n10 00AK      ((530417 191627.4, 530416.8 191627.6, 530410 191631.2, 530396.3 19…\n# ℹ 23 more rows\n\ndistricts[1,1] #read first row, first column: 00AA\n\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 2\n  DIST_CODE                                                             geometry\n  &lt;chr&gt;                                                            &lt;POLYGON [m]&gt;\n1 00AA      ((531028.5 181611.2, 531036.1 181611.5, 531074 181610.3, 531107 181…\n\n# variable can be called using the operator $\ndistricts$DIST_NAME #read the column \"DIST_NAME\"\n\n [1] \"City of London\"         \"Barking and Dagenham\"   \"Barnet\"                \n [4] \"Bexley\"                 \"Brent\"                  \"Bromley\"               \n [7] \"Camden\"                 \"Croydon\"                \"Ealing\"                \n[10] \"Enfield\"                \"Greenwich\"              \"Hackney\"               \n[13] \"Hammersmith and Fulham\" \"Haringey\"               \"Harrow\"                \n[16] \"Havering\"               \"Hillingdon\"             \"Hounslow\"              \n[19] \"Islington\"              \"Kensington and Chelsea\" \"Kingston upon Thames\"  \n[22] \"Lambeth\"                \"Lewisham\"               \"Merton\"                \n[25] \"Newham\"                 \"Redbridge\"              \"Richmond upon Thames\"  \n[28] \"Southwark\"              \"Sutton\"                 \"Tower Hamlets\"         \n[31] \"Waltham Forest\"         \"Wandsworth\"             \"Westminster\"           \n\n\nWe can read or create subsets:\n\n# dataframe can be subsetted using conditional statement\n# read the rows which have \"City of London\" as value for DIST_NAME\ndistricts[districts$DIST_NAME== \"City of London\",] \n\nSimple feature collection with 1 feature and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 530966.7 ymin: 180404.3 xmax: 533842.7 ymax: 182198.4\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 1 × 3\n  DIST_CODE DIST_NAME                                                   geometry\n  &lt;chr&gt;     &lt;chr&gt;                                                  &lt;POLYGON [m]&gt;\n1 00AA      City of London ((531028.5 181611.2, 531036.1 181611.5, 531074 18161…\n\n\n\n\n\n\n\n\nNote\n\n\n\nGo back to open science for subsetting with dplyr.\n\n\n\n\nQuick visualisation\nLet’s start by plotting London in a colour and adding Hackney (a district) in a different colour.\n\n# plot london in grey\nplot(districts$geometry, col = \"lightgrey\")\n\n# Add city of London in turquoise to the map\nplot(districts[districts$DIST_NAME == \"Hackney\", ]$geometry, # select city of london\n     col = \"turquoise\",\n     add = T) # add to the existing map\n\n\n\n\n\n\n\n\nSome guidance on colours in R can be found here.\nHow to reset a plot:\n\nplot(districts$geometry, reset = T) # reset",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#styling-plots",
    "href": "spatialdataR.html#styling-plots",
    "title": "Lab in R",
    "section": "Styling plots",
    "text": "Styling plots\nIt is possible to tweak many aspects of a plot to customize if to particular needs. In this section, we will explore some of the basic elements that will allow us to obtain more compelling maps.\nNote: some of these variations are very straightforward while others are more intricate and require tinkering with the internal parts of a plot. They are not necessarily organized by increasing level of complexity.\n\nPlotting different layers\nWe first start by plotting one layer over another\n\nplot(districts$geometry)\nplot(a_roads$geometry, add=T) # note the `add=T` is adding the second layer.\n\n\n\n\n\n\n\n\nOr use the ggplot package for something a bit fancier\n\nggplot() +\n geom_sf(data = districts, color = \"black\") +  # Plot districts with black outline\n  geom_sf(data = a_roads, color = \"brown\") +  # Plot roads with brown color and 50% transparency\n  theme_minimal() \n\n\n\n\n\n\n\n\n\n\nChanging transparency\nThe intensity of color of a polygon can be easily changed through the alpha attribute in plot. This is specified as a value betwee zero and one, where the former is entirely transparent while the latter is the fully opaque (maximum intensity):\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nRemoving axes\nAlthough in some cases, the axes can be useful to obtain context, most of the times maps look and feel better without them. Removing the axes involves wrapping the plot into a figure, which takes a few more lines of aparently useless code but that, in time, it will allow you to tweak the map further and to create much more flexible designs.\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme(line = element_blank(), # remove tick marks\n        rect = element_blank(), # remove background\n        axis.text=element_blank()) # remove x and y axis\n\n\n\n\n\n\n\n  # theme_void() # could also be used instead of the 3 above lines \n\nFor more on themes in ggplot see here\n\n\nAdding a title\nAdding a title is an extra line, if we are creating the plot within a figure, as we just did. To include text on top of the figure:\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  # Plot districts with black outline & no fill (NA)\n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) +  # Plot roads with brown color and 50% transparency\n  theme_void() + # \n  ggtitle(\"Some London roads\") #add ggtitle\n\n\n\n\n\n\n\n\n\n\nChanging what border lines look like\nBorder lines sometimes can distort or impede proper interpretation of a map. In those cases, it is useful to know how they can be modified. Let us first see the code to make the lines thicker and black, and then we will work our way through the different steps:\n\nggplot() +\n  geom_sf(data = districts, fill = NA, color = \"black\") +  \n  geom_sf(data = a_roads, color = \"brown\", alpha = 0.5) + \n  geom_sf(data = poi_sf, color = \"blue\", size = 3) + # size adjusts size of visualization\n  theme_void() +\n  ggtitle(\"Some London Roads\") #add ggtitle\n\n\n\n\n\n\n\n\n\n\nLabelling\nLabeling maps is of paramount importance as it is often key when presenting data analysis and visualization. Properly labeled maps enables readers to effectively analyze and interpret spatial data.\nHere we are using geom_sf_text to add data, specifically the distrct name, to the centre of each District in a specific size.\n\nggplot() +\n  geom_sf(data = districts,\n          fill = \"gray95\") +\n  geom_sf_text(data = districts,\n               aes(label = DIST_NAME),\n               fun.geometry = sf::st_centroid, size=2) +\n  theme_void()\n\n\n\n\n\n\n\n\ngeom_sf_text() and geom_sf_label() can also be used to achieve similar effects.",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#coordinate-reference-systems",
    "href": "spatialdataR.html#coordinate-reference-systems",
    "title": "Lab in R",
    "section": "Coordinate reference Systems",
    "text": "Coordinate reference Systems\n\nCRSs in R\nCoordindate reference systems (CRS) are the way geographers and cartographers represent a three-dimentional objects, such as the round earth, on a two-dimensional plane, such as a piece of paper or a computer screen. If the source data contain information on the CRS of the data, we can modify this.\nFirst we need to retrieve the CRS from the vector data.\n\nst_crs(districts) # retrieve coordinate reference system from object\n\nCoordinate Reference System:\n  User input: OSGB36 / British National Grid \n  wkt:\nPROJCRS[\"OSGB36 / British National Grid\",\n    BASEGEOGCRS[\"OSGB36\",\n        DATUM[\"Ordnance Survey of Great Britain 1936\",\n            ELLIPSOID[\"Airy 1830\",6377563.396,299.3249646,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4277]],\n    CONVERSION[\"British National Grid\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",49,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-2,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996012717,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",400000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-100000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United Kingdom (UK) - offshore to boundary of UKCS within 49°45'N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\"],\n        BBOX[49.75,-9.01,61.01,2.01]],\n    ID[\"EPSG\",27700]]\n\n\nThe st_crs function also has one helpful feature - we can retrieve some additional information about the used CRS. For example, try to run:\n\nst_crs(districts)$IsGeographic # to check is the CRS is geographic or not\n\n[1] FALSE\n\nst_crs(districts)$units_gdal # to find out the CRS units\n\n[1] \"metre\"\n\nst_crs(districts)$srid # extracts its SRID (when available)\n\n[1] \"EPSG:27700\"\n\nst_crs(districts)$proj4string # extracts the proj4string representation\n\n[1] \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs\"\n\n\nAs we can see, there is information stored about the reference system: it is using the standard British projection (British National Grid), which is expressed in meters. There are also other less decipherable parameters but we do not need to worry about them right now.\nIf we want to modify this and “reproject” the polygons into a different CRS, the quickest way is to find the EPSG code online (epsg.io is a good one, although there are others too). For example, if we wanted to transform the dataset into lat/lon coordinates, we would use its EPSG code, 4326 (CRS’s name “WGS84”):\nIn cases when a coordinate reference system (CRS) is missing or the wrong CRS is set, the st_set_crs() function can be used:\n\ndistricts_4326 = st_transform(districts, \"EPSG:4326\") # set CRS\n# districts_4326 &lt;- st_transform(districts_4326, crs = 4326)\n\n\n\nFrom coordinates to spatial objects\nCRSs are also very useful if we obtain data that is in a csv, has coordinates but needs to be transformed to a spatial dataframe. For example we have some London housing transactions we want to import and use.\nWe want to transform the .csv in a sf object with the st_as_sf function using the coordinates stored in columns 17 and 18, and then we set the dataframe CRS to the British National Grid (EPSG:27700) using the st_set_crs function.\n\nhousesales &lt;- read.csv(\"data/London/Tables/housesales.csv\") # import housesales data from csv\n\n# 3 commands: \nhousesales_filtered = filter(housesales,price &lt; 500000)\nhousesales_sf &lt;- st_as_sf(housesales_filtered, coords = c(17,18)) # denote columns which have the coordinates\nhousesales_clean &lt;- st_set_crs(housesales_sf, 27700)# set crs to British National Grid \n\nAs we’ve seen in open science, we can do consecutive operations using dplyr pipes %&gt;%, they are used to simplify syntax. Pipes allow to perform successive operations on dataframes in one command! More info here.\n\n# all one in go and one output\nhousesales_clean = housesales %&gt;% # select the main object\n  filter(price &lt; 500000) %&gt;% # remove values above 500,000\n  st_as_sf(coords = c(17,18)) %&gt;% # # denote columns which have the coordinates\n  st_set_crs(27700) # set crs to British National Grid\n\n\n\nZooming in or out\nIt’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\nHere for example we are zooming in to some of the point we created at the beginning of the lab.\n\nggplot() + \n geom_sf(data = districts_4326$geometry) + \n  geom_sf(data = poi_sf$geometry, fill = 'blue', size = 3) + \n  coord_sf(xlim = c(-0.180723,-0.014212), ylim = c(51.476668,51.532337)) +\n   theme_void()",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#manipulating-spatial-tables",
    "href": "spatialdataR.html#manipulating-spatial-tables",
    "title": "Lab in R",
    "section": "Manipulating Spatial Tables",
    "text": "Manipulating Spatial Tables\nOnce we have an understanding of how to visually display spatial information contained, let us see how it can be combined with the operations related to manipulating non-spatial tabular data. Essentially, the key is to realize that a geographical dataframes contain most of its spatial information in a single column named geometry, but the rest of it looks and behaves exactly like a non-spatial dataframes (in fact, it is). This concedes them all the flexibility and convenience that we saw in manipulating, slicing, and transforming tabular data, with the bonus that spatial data is carried away in all those steps. In addition, geo dataframes also incorporate a set of explicitly spatial operations to combine and transform data. In this section, we will consider both.\nGeo dataframes come with a whole range of traditional GIS operations built-in. Here we will run through a small subset of them that contains some of the most commonly used ones.\n\nAreaLengthCentroidsBuffers and selecting by location\n\n\nOne of the spatial aspects we often need from polygons is their area. “How big is it?” is a question that always haunts us when we think of countries, regions, or cities. To obtain area measurements, first make sure the dataframe you are working with is projected. If that is the case, you can calculate areas as follows:\nWe had already checked that district was projected to the British National Grid\n\ndistricts &lt;- districts %&gt;%\n  mutate(area = st_area(.)/1000000) # calculate area and make it km2\n\n\n\nSimilarly, an equally common question with lines is their length. Also similarly, their computation is relatively straightforward, provided that our data are projected.\n\na_roads &lt;- a_roads %&gt;%\n  mutate(street_length = st_length(geometry)) # calculate street length in metres\n\nIf you check the dataframe you will see the lengths.\n\n\nSometimes it is useful to summarize a polygon into a single point and, for that, a good candidate is its centroid (almost like a spatial analogue of the average).\n\n# Create a dataframe with centroids\ncentroids_df &lt;- districts %&gt;%\n  st_centroid()\n\nPlot the centroids\n\nggplot() +\n  geom_sf(data = districts) +  # Plot the districts segments\n  geom_sf(data = centroids_df, color = \"red\", size = 2) +  # Plot the centroids in red\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHere, we first select by expression the Hackney district and then we create a 1km buffer around it with the st_buffer() function from the sf package.\n\n# buffer\ncentroid_buffers &lt;- st_buffer(centroids_df, 1000)\n\nggplot() +\n  geom_sf(data = districts) +  # Plot the districts segments\n  geom_sf(data = centroids_df, color = \"red\", size = 2) +  # Plot the centroids in red\n  geom_sf(data = centroid_buffers, color = \"darkred\", size = 2) +  # Plot the buffers of the centroids\n  theme_minimal()",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#joins",
    "href": "spatialdataR.html#joins",
    "title": "Lab in R",
    "section": "Joins",
    "text": "Joins",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#join-districts-with-educational-level-data",
    "href": "spatialdataR.html#join-districts-with-educational-level-data",
    "title": "Lab in R",
    "section": "Join districts with educational level data",
    "text": "Join districts with educational level data\n\n# import qualifications data from csv\nqualifications2001_df &lt;- read.csv(\"data/London/Tables/qualifications2001_2.csv\")\n\n# take a quick look at the table by reading the first 5 lines\nhead(qualifications2001_df)\n\n  Zone_Code            Zone_Name Population1674 Noquals Level1 Level2 Level3\n1      00AA       City of London           6067     607    359    634    665\n2      00AB Barking and Dagenham         113579   44873  21654  20564   6626\n3      00AC               Barnet         228123   44806  25558  41118  24695\n4      00AD               Bexley         156172   44887  32110  35312  10759\n5      00AE                Brent         198712   48915  23913  33280  21121\n6      00AF              Bromley         212368   47093  34879  48012  19550\n  Level4\n1   3647\n2  11615\n3  80907\n4  20704\n5  60432\n6  49598\n\n\n\nUsing the dplyr package which is a part of tidyverse you can:\nJoin merge two datasets join(x, y).\n\nleft_join returns all rows from x (districts), and all columns from x (districts) and y (qualifications2001)\ninner join returns all rows from x where there are matching values in y, and all columns from x and y)\nright join returns all rows from x, and all columns from x and y)\nfull_join returns all rows and all columns from both x and y)\n\nMerge the data from the districts shapefile and the qualifications from the csv file\nJoin districts data to qualifications2001 using district identifiers called DIST_CODE in districts and Zone_Code in qualifications2001_df\n\n\n#join\ndistricts &lt;- left_join(districts, \n                       qualifications2001_df, \n                       by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# tidyverse alternative with pipe operator %&gt;%\n\ndistricts_tidy &lt;- districts %&gt;%\n  left_join(qualifications2001_df, by=c(\"DIST_CODE\"=\"Zone_Code\"))\n\n# check the first rows of the merged data table\nhead(districts)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 515484.9 ymin: 156480.8 xmax: 554503.8 ymax: 198355.2\nProjected CRS: OSGB36 / British National Grid\n# A tibble: 6 × 11\n  DIST_CODE DIST_NAME                   geometry   area Zone_Name Population1674\n  &lt;chr&gt;     &lt;chr&gt;                  &lt;POLYGON [m]&gt;  [m^2] &lt;chr&gt;              &lt;int&gt;\n1 00AA      City of L… ((531028.5 181611.2, 531…   3.15 City of …           6067\n2 00AB      Barking a… ((550817 184196, 550814 …  37.8  Barking …         113579\n3 00AC      Barnet     ((526830.3 187535.5, 526…  86.7  Barnet            228123\n4 00AD      Bexley     ((552373.5 174606.9, 552…  64.3  Bexley            156172\n5 00AE      Brent      ((524661.7 184631, 52466…  43.2  Brent             198712\n6 00AF      Bromley    ((533852.2 170129, 53385… 150.   Bromley           212368\n# ℹ 5 more variables: Noquals &lt;int&gt;, Level1 &lt;int&gt;, Level2 &lt;int&gt;, Level3 &lt;int&gt;,\n#   Level4 &lt;int&gt;\n\n\n\nCalculation\nNow, let’s create the share of people with level 4 qualification, i.e. create the new variable Level4p equal to the number of people with level4 qualification divided by total population:\n\ndistricts &lt;- districts %&gt;%\n  mutate(Level4p = Level4/Population1674)",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#saving-maps-to-figures",
    "href": "spatialdataR.html#saving-maps-to-figures",
    "title": "Lab in R",
    "section": "Saving maps to figures",
    "text": "Saving maps to figures\nCreate a file to put your maps:\n\ndir.create(\"maps\") \n\nIf you were creating a map with teh plot function you could save it like this:\n\npdf(\"maps/london_test.pdf\") # Opening the graphical device\nplot(districts$geometry)\nplot(housesales_clean$geometry, add=TRUE) \ndev.off() # Closing the graphical device\n\nquartz_off_screen \n                2 \n\n\nLet’s create a simple map with the variable we just created:\n\ntest_map &lt;- ggplot() \n  geom_sf(data = districts, aes(fill = Level4p)) +\n  theme_void() \n\nNULL\n\n\nLet’s save it, as you can see you can play around with the formatting. For more on ggsave have a look here\n\nggsave(\"maps/map3.pdf\")\n\nSaving 7 x 5 in image\n\nggsave(\"maps/test_map_1.png\", width = 4, height = 4)\nggsave(\"maps/test_map_2.png\", width = 20, height = 20, units = \"cm\")",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#adding-baselayers",
    "href": "spatialdataR.html#adding-baselayers",
    "title": "Lab in R",
    "section": "Adding baselayers",
    "text": "Adding baselayers\nVarious R libraries allow us to add static basemaps to out maps. We will be using the base_map() function to(down)load a basemap in our maps. This is from the library(basemapR) which is easy to execute.\nThe style of basemap currently supported are ‘dark’, ‘hydda’, ‘positron’, ‘voyager’, ‘wikimedia’, ‘mapnik’, google, google-nobg, google-hybrid, google-terrain, google-satellite, google-road. The package aims to ease the use of basemaps in different contexts by providing a function interface as minimalist as possible. There are other packages which support more choices like library(basemaps) which you can check out here\nWe simply add base_map () to our ggplot:\n\nggplot() +\n  base_map(st_bbox(districts_4326), increase_zoom = 2) + \n  geom_sf(data = districts_4326, fill = NA)\n\nattribution: &copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors &copy; &lt;a href=\"https://carto.com/attributions\"&gt;CARTO&lt;/a&gt;\n\n\n\n\n\n\n\n\n\nIf we want to specify the map we use basemap =:\n\nggplot() +\n  base_map(st_bbox(districts_4326), basemap = 'google-terrain', increase_zoom = 2) +\n  geom_sf(data = districts_4326, fill = NA) +\n  geom_sf(data = poi_sf) +\n  ggthemes::theme_map()\n\nplease cite: map data © 2020 Google",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "spatialdataR.html#interactive-maps",
    "href": "spatialdataR.html#interactive-maps",
    "title": "Lab in R",
    "section": "Interactive maps",
    "text": "Interactive maps\nEverything we have seen so far relates to static maps. These are useful for publication, to include in reports or to print. However, modern web technologies afford much more flexibility to explore spatial data interactively.\nIn this example, ee will use the package leaflet. This integration connects us with the popular web mapping library Leaflet.js. The key part of the code below is addProviderTiles, We are using CartoDB.Positron but there are many more that you can explore here.\n\nlibrary(leaflet)\npopup = c(\"The British Museum\", \"Big Ben\", \"King's Cross\", \"The Natural History Museum\")\nleaflet() %&gt;%\n  addProviderTiles(\"CartoDB.Positron\") %&gt;%\n  addMarkers(lng = c(-0.1459604, -0.1272057, -0.1319481, -0.173734),\n             lat = c(51.5045975, 51.5007325, 51.5301701, 51.4938451), \n             popup = popup)",
    "crumbs": [
      "2 Spatial Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvector.html",
    "href": "mapvector.html",
    "title": "3 Mapping Vector Data",
    "section": "",
    "text": "Geovisualisation\nThis section is all about Geovisualisation and displaying statistical information on maps. We start with an introduction on what geovisualisation is; then we follow with the modifiable areal unit problem (MAUP), a key concept to keep in mind when displaying statistical information spatially; and we wrap up with tips to make awesome choropleths, thematic maps.\nSlides can be downloaded “here”\nGeovisualisation is an area that underpins much what we will discuss in this course. Often, we will be presenting the results of more sophisticated analyses as maps. So getting the principles behind mapping right is critical. In this clip, we cover what is (geo)visualisation and why it is important.",
    "crumbs": [
      "3 Mapping Vector Data"
    ]
  },
  {
    "objectID": "mapvector.html#geographical-containers-for-data",
    "href": "mapvector.html#geographical-containers-for-data",
    "title": "3 Mapping Vector Data",
    "section": "Geographical containers for data",
    "text": "Geographical containers for data\nThis section tries to get you to think about the geographical containers we use to represent data in maps. By that, we mean the areas, delineations and aggregations we, implicitly or explicitly, incur in when mapping data. This is an important aspect, but Geographers have been aware of them for a long time, so we are standing on the shoulders of giants.",
    "crumbs": [
      "3 Mapping Vector Data"
    ]
  },
  {
    "objectID": "mapvector.html#choropleths",
    "href": "mapvector.html#choropleths",
    "title": "3 Mapping Vector Data",
    "section": "Choropleths",
    "text": "Choropleths\nChoropleths are thematic maps and, these days, are everywhere. From elections, to economic inequality, to the distribution of population density, there’s a choropleth for everyone. Although technically, it is easy to create choropleths, it is even easier to make bad choropleths. Fortunately, there are a few principles that we can follow to create effective choropleths. Get them all delivered right to the conform of your happy place in the following clip and slides!",
    "crumbs": [
      "3 Mapping Vector Data"
    ]
  },
  {
    "objectID": "mapvector.html#further-readings",
    "href": "mapvector.html#further-readings",
    "title": "3 Mapping Vector Data",
    "section": "Further readings",
    "text": "Further readings\n\nCynthia Brewer’s “Designing Better Maps” covers several core aspects of building effective geovisualisations.\nChoropleth Mapping chapter by Rey, Arribas-Bel and Wolf",
    "crumbs": [
      "3 Mapping Vector Data"
    ]
  },
  {
    "objectID": "mapvectorR.html",
    "href": "mapvectorR.html",
    "title": "Lab in R",
    "section": "",
    "text": "Choropleths\nIn this session, we will build on all we have learnt so far about loading and manipulating (spatial) data and apply it to one of the most commonly used forms of spatial analysis: choropleths. Remember these are maps that display the spatial distribution of a variable encoded in a color scheme, also called palette. Although there are many ways in which you can convert the values of a variable into a specific color, we will focus in this context only on a handful of them, in particular:",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#choropleths",
    "href": "mapvectorR.html#choropleths",
    "title": "Lab in R",
    "section": "",
    "text": "Unique values\nEqual interval\nQuantiles\nFisher-Jenks",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#installing-packages",
    "href": "mapvectorR.html#installing-packages",
    "title": "Lab in R",
    "section": "Installing Packages",
    "text": "Installing Packages\nBefore all this mapping fun, let us get the importing of libraries and data loading out of the way:\n\n# Load the 'sf' library, which stands for Simple Features, used for working with spatial data.\nlibrary(sf)\n# Load the 'tidyverse' library, a collection of packages for data manipulation and visualization.\nlibrary(tidyverse)\n# Load the 'tmap' library, which is used for creating thematic maps and visualizing spatial data.\nlibrary(tmap)\n# The 'readr' library provides a fast and user-friendly way to read data from common formats like CSV.\nlibrary(readr)\n# Converts Between GeoJSON and simple feature objects\nlibrary(geojsonsf) \n# RColorBrewer library for creating visually appealing color schemes for plots and data visualizations\nlibrary(RColorBrewer)\n# Corking with class intervals and classification methods, esp in the context of spatial data analysis.\nlibrary(classInt)",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#data",
    "href": "mapvectorR.html#data",
    "title": "Lab in R",
    "section": "Data",
    "text": "Data\nWe will be using World Bank data for this section, looking at World Development Indicators and Education Statistics. We will be focusing on the Middle East and North Africa (MENA). We start by loading the relevant geometries:\n\nmena_sf &lt;- geojson_sf(\"data/MENA/MENA.geojson\") # we load the geojson using `geojson_sf`\n\nplot(mena_sf$geometry) # we plot the geometry to make sure it looks like it should\n\n\n\n\n\n\n\n\nDon’t forget that before you go further, you want to check the CRS of the sf object as well as the dataframe.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nst_crs(mena_sf)\n\nCoordinate Reference System:\n  User input: 4326 \n  wkt:\nGEOGCS[\"WGS 84\",\n      DATUM[\"WGS_1984\",\n        SPHEROID[\"WGS 84\",6378137,298.257223563,\n          AUTHORITY[\"EPSG\",\"7030\"]],\n        AUTHORITY[\"EPSG\",\"6326\"]],\n      PRIMEM[\"Greenwich\",0,\n        AUTHORITY[\"EPSG\",\"8901\"]],\n      UNIT[\"degree\",0.0174532925199433,\n        AUTHORITY[\"EPSG\",\"9122\"]],\n      AXIS[\"Latitude\",NORTH],\n      AXIS[\"Longitude\",EAST],\n    AUTHORITY[\"EPSG\",\"4326\"]]\n\nhead(mena_sf)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 34.24835 ymin: 22.62095 xmax: 63.31963 ymax: 39.77153\nGeodetic CRS:  WGS 84\n                  name              formal_en_name code_a2 code_a3\n1 United Arab Emirates        United Arab Emirates      AE     ARE\n2              Bahrain          Kingdom of Bahrain      BH     BHR\n3                 Iran    Islamic Republic of Iran      IR     IRN\n4                 Iraq            Republic of Iraq      IQ     IRQ\n5               Israel             State of Israel      IL     ISR\n6               Jordan Hashemite Kingdom of Jordan      JO     JOR\n                        geometry\n1 MULTIPOLYGON (((53.86305 24...\n2 POLYGON ((50.55161 26.19424...\n3 MULTIPOLYGON (((55.05437 25...\n4 POLYGON ((42.89674 37.32491...\n5 POLYGON ((35.80363 33.24846...\n6 POLYGON ((39.04633 32.30849...\n\n\n\n\n\nWe then load the csv with some World Development Indicators data.\n\nworld_dev &lt;- read.csv(\"data/MENA/mena_worlddevelop.csv\") \n\nAnd join the two objects using the relevant codes.\n\nworld_dev_sf &lt;- left_join(mena_sf, \n                       world_dev, \n                       by=c(\"code_a3\"=\"Country.Code\"))\n\nNow we are fully ready to map!\nWe will be using two packages throughout the module. Both tmap (see here), and ggplot2 (, see here). There is also mapsf (for thematic cartography, see here) as another alternative.",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#unique-values",
    "href": "mapvectorR.html#unique-values",
    "title": "Lab in R",
    "section": "Unique values",
    "text": "Unique values\nA choropleth for categorical variables simply assigns a different color to every potential value in the series. Variables could be both nominal or ordinal.\nNominal: Nominal variables represent categories or labels without any inherent order or ranking. The categories are distinct and do not have a natural progression or hierarchy, such as “apple,” “banana,” and “orange” for fruit types.\nOrdinal : Ordinal variables represent categories or labels with a meaningful order or ranking. The relative order or hierarchy among the categories is significant, indicating a clear progression from lower to higher values, such as “low,” “medium,” and “high” for satisfaction levels.\nIn R, creating categorical choropleths is possible with one line of code. To demonstrate this, we can plot the the income level of countries in the MENA region (coded in our table as the income_group variable). The code below uses ggplot and tmap to plot.\n\ntmapggplot\n\n\ntmap provides another option using the function tm_fill and tm_shape. A line has been added to to start editing the placement of the legend as well with tm_layout.\n\ntm_shape(world_dev_sf) + # data\n  tm_fill(\"income_group\", title = \"Income Groups\")+ # variable and giving a title\n  tm_borders() + # add borders\n  tm_layout(legend.outside = TRUE, legend.outside.position = \"bottom\") # placing the legend\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_fill()`: migrate the argument(s) related to the legend of the\nvisual variable `fill` namely 'title' to 'fill.legend = tm_legend(&lt;HERE&gt;)'\n\n\n\n\n\n\n\n\n\n\n\nThis is similar in ggplot. geom_sf is calling the geometric object and fill is defining what values we want to fill the polygons with.\n\nggplot(data = world_dev_sf) +\n    geom_sf(aes(fill = income_group)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThese maps are all a bit rough a need quite a bit more work. They are just a starting point.",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#equal-interval",
    "href": "mapvectorR.html#equal-interval",
    "title": "Lab in R",
    "section": "Equal Interval",
    "text": "Equal Interval\nIf, instead of categorical variables, we want to display the geographical distribution of a continuous phenomenon, we need to select a way to encode each value into a color. One potential solution is applying what is usually called “equal intervals”. The intuition of this method is to split the range of the distribution, the difference between the minimum and maximum value, into equally large segments and to assign a different color to each of them according to a palette that reflects the fact that values are ordered.\nCreating the choropleth is relatively straightforward in R. For example, to create an equal interval of GDP per capita in 2015 (v_2015).\nFirst we need to prepare the data, going back to our data wrangling.\n\nworld_dev_filtered &lt;- world_dev_sf %&gt;%\n  # Step 1: Filter rows where Series.Name is \"GDP per capita, PPP (current international $)\"\n  filter(Series.Name == \"GDP per capita, PPP (current international $)\") %&gt;% \n  # Step 2: Further filter out rows where 'v_2015' is not missing (i.e., remove NA values)\n  filter(!is.na(v_2015)) %&gt;% \n  # Step 3: Mutate (modify) the 'v_2015' variable by rounding it to a whole number\n  mutate(v_2015 = round(as.numeric(v_2015))) \n\n\n\n\n\n\n\nAdvanced - Looping through different columns\n\n\n\n\n\nYou can also loop through different columns, is for example you wanted to convert GDP per capita for all the years to numeric.\n\n# All the columns to convert\ncolumns_to_convert &lt;- c(\"v_2010\", \"v_2015\", \"v_2020\")\n\n# Loop through the columns and convert to numeric\nfor (col in columns_to_convert) {\n  world_dev_sf[[col]] &lt;- as.numeric(world_dev_sf[[col]])\n}\n\n\n\n\nNow let’s map using equal intervals.\n\ntmapggplot\n\n\nWith a few easy functions tmap allows you to plot your data with different styles. Here we are using equal for equal intervals. Have a look at the code annotation for more detail.\n\ntm_basemap() +\n# Create a basic map using the tm_basemap() function.\n  tm_shape(world_dev_filtered) + \n# Define the data source and shape to be used for the map using tm_shape().\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"name\", n = 7, style= \"equal\") +\n# Add polygons to the map using the tm_polygons() function.\n# 'v_2015bis' is our variable within the 'world_dev_filtered' dataset.\n# The palette \"YlGn\" specifies the color palette for the polygons.\n# 'id' is set to \"name,\" which means the 'name' column will be used to identify polygons.\n# 'n' is set to 7, which means the data will be divided into 7 classes.\n# 'style' is set to \"equal,\" which indicates equal interval classification for the data.\n    tm_layout(\n    legend.outside = TRUE, legend.outside.position = \"bottom\",\n    title = \"GDP per capita by Equal Interval Classification\")\n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"equal\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[v3-&gt;v4] `tm_layout()`: use `tm_title()` instead of `tm_layout(title = )`\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"YlGn\" is named\n\"brewer.yl_gn\"\nMultiple palettes called \"yl_gn\" found: \"brewer.yl_gn\", \"matplotlib.yl_gn\". The first one, \"brewer.yl_gn\", is returned.\n\n\n\n\n\n\n\n\n# Customize the map layout using tm_layout().\n\nOf course, this could do with some further work. You might want to check out tm_layout\n\n\nMapping in ggplot can be a bit tricky at the beginning. You will want to take a look at the package classInt here.\nStep 1: Using the dplyr pipe operator %&gt;%, we’re modifying the world_dev_filtered data frame. We’re using the mutate function to create a new column ‘v_2015bis’ in ‘world_dev_filtered’.\nStep 2: The ‘v_2015’ column in ‘world_dev_filtered’ is being divided by 1000 so we can talk about thousands of $ in GDP per capita, and the result is being rounded to the nearest integer.\n\nworld_dev_filtered &lt;- world_dev_filtered %&gt;%\n  mutate(v_2015bis = round(v_2015 / 1000))\n\nStep 3: Calculate equal interval breaks with function classIntervals and store them.\n\ne_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 7, style = \"equal\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$e_breaks &lt;- cut(world_dev_filtered$v_2015bis, e_breaks$brks)\n\n\ne_breaks: This is a variable name that you are assigning to store the result of the class intervals calculation.\nclassIntervals(): This is a function that calculates class intervals for a given numeric vector. It is likely provided by a package like ‘classInt’ or ‘classIntervals’ in R.\nworld_dev_filtered$v_2015: This is the data vector that you want to create class intervals for. It appears to be a column named ‘v_2015’ within the ‘world_dev_filtered’ dataset. n = 7: This argument specifies that you want to divide the data into 7 classes. style = \"equal\": This argument specifies that you want to use equal interval classification, which means that the data range will be divided into equal-sized intervals. These class intervals can be useful for creating data visualizations like choropleth maps or histograms.\n\nStep 4: Finally we can map! as you see a bit of extra work was needed, but you ultimately have more control.\n\nnum_bins &lt;-7\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = e_breaks)) +\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$e_breaks), perl = TRUE)))  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels\n  ) +\n  labs(\n    title = \"GDP per capita by Equal Interval Classification\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nIt is important to understand that equal intervals can first and foremost be visualised on the data distribution. We have already created these intervals with the function classIntervals in Step 3 of the ggplot tab above. Here we need a couple of extra steps to collect the break values and plot them in histogram form.\n\n\n\n\n\n\nClass Interval styles\n\n\n\nThe function classIntervals has the following styles: “fixed”, “sd”, “equal”, “pretty”, “quantile”, “kmeans”, “hclust”, “bclust”, “fisher”, “jenks”, “dpih”, “headtails”, “maximum”, or “box”.\n\n\n\n# Same step as above\ne_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 7, style = \"equal\")\nworld_dev_filtered$e_breaks &lt;- cut(world_dev_filtered$v_2015, e_breaks$brks)\n\n# Collect the values of the breaks\ne_break_values &lt;- e_breaks$brks\n\n# Place the values in a dataframe\ne_break_values_df &lt;- data.frame(BreakValues = e_break_values)\n\n# Create a ggplot2 visualization with 'world_dev_filtered' dataset as the data source\n# and 'v_2015' as the variable for the x-axis.\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Add a density plot to the visualization with fill color set to dark blue\n# and transparency (alpha) set to 0.4.\n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) along the x-axis with transparency (alpha) set to 0.5.\n  geom_rug(alpha = 0.5) +\n# Add vertical lines to the plot based on the 'e_break_values_df' dataset\n# with x-intercepts specified by the 'BreakValues' variable.\n# The color of these lines is set to green.\n  geom_vline(data = e_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n# Apply the 'theme_minimal()' theme to the plot for a minimalistic appearance.\n  theme_minimal() +\n# Modify the x-axis label to display \"GDP per capita in 2015\".\n  labs(x = \"GDP per capita in 2015\")\n\n\n\n\n\n\n\n\nTechnically speaking, the figure is created by overlaying a KDE plot with vertical bars for each of the break points. This makes much more explicit the issue highlighted by which the first two bin contain a large amount of observations while the one with top values only encompasses a handful of them.",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#quantiles",
    "href": "mapvectorR.html#quantiles",
    "title": "Lab in R",
    "section": "Quantiles",
    "text": "Quantiles\nOne solution to obtain a more balanced classification scheme is using quantiles. This, by definition, assigns the same amount of values to each bin: the entire series is laid out in order and break points are assigned in a way that leaves exactly the same amount of observations between each of them. This “observation-based” approach contrasts with the “value-based” method of equal intervals and, although it can obscure the magnitude of extreme values, it can be more informative in cases with skewed distributions.\nThe code required to create the choropleth mirrors that needed above for equal intervals:\n\ntmapggplot\n\n\nAs in our previous example, the coding is a bit simpler if we use tmap.\n\ntm_basemap() +\n  tm_shape(world_dev_filtered) +\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"Country.Name\", n = 4, style= \"quantile\")   \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"quantile\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"YlGn\" is named\n\"brewer.yl_gn\"\nMultiple palettes called \"yl_gn\" found: \"brewer.yl_gn\", \"matplotlib.yl_gn\". The first one, \"brewer.yl_gn\", is returned.\n\n\n\n\n\n\n\n\n\n\n\nAs before, first we create the intervals, in this case quantiles.\n\n# Find quantile breaks for data segmentation into four groups.\nqt_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 4, style = \"quantile\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$qt_breaks &lt;- cut(world_dev_filtered$v_2015bis, qt_breaks$brks)\n\nThen we map the data:\n\nnum_bins &lt;-4\n# Define a color palette for visualizing data.\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\n# plot\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = qt_breaks)) +\ntheme_void() + # remove x and y axis\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$qt_breaks), perl = TRUE)))) +  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels \n  labs(\n    title = \"GDP per capita (Quantiles)\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nAs we are dealing with a limited number of countries it is easier to see how the data is being divided differently in the histogram.\n\nqt_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 4, style = \"quantile\")\nworld_dev_filtered$qt_breaks &lt;- cut(world_dev_filtered$v_2015, qt_breaks$brks)\n\n# Collect the values of the breaks\nqt_break_values &lt;- qt_breaks$brks\n# Place the values in a dataframe\nqt_break_values_df &lt;- data.frame(BreakValues = qt_break_values)\n\n# Create a ggplot2 visualization\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Density plot \n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) \n  geom_rug(alpha = 0.5) +\n# Add vertical lines at 'BreakValues' \n  geom_vline(data = qt_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n  theme_minimal() +\n  labs(x = \"GDP per capita in 2015\")",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#fisher-jenks",
    "href": "mapvectorR.html#fisher-jenks",
    "title": "Lab in R",
    "section": "Fisher-Jenks",
    "text": "Fisher-Jenks\nEqual interval and quantiles are only two examples of very many classification schemes to encode values into colors. As an example of a more sophisticated one, let us create a Fisher-Jenks choropleth:\n\ntmapggplot\n\n\nAs in our previous example, the coding is a bit simpler if we use tmap.\n\ntm_basemap() +\n  tm_shape(world_dev_filtered) +\n  tm_polygons(\"v_2015\", palette = \"YlGn\", id = \"Country.Name\", n = 7, style= \"fisher\")   \n\n\n\n\n── tmap v3 code detected ───────────────────────────────────────────────────────\n\n\n[v3-&gt;v4] `tm_polygons()`: instead of `style = \"fisher\"`, use fill.scale =\n`tm_scale_intervals()`.\nℹ Migrate the argument(s) 'style', 'n', 'palette' (rename to 'values') to\n  'tm_scale_intervals(&lt;HERE&gt;)'\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"YlGn\" is named\n\"brewer.yl_gn\"\nMultiple palettes called \"yl_gn\" found: \"brewer.yl_gn\", \"matplotlib.yl_gn\". The first one, \"brewer.yl_gn\", is returned.\n\n\n\n\n\n\n\n\n\n\n\nAs before, first we create the intervals, in this case fisher jenks\n\n# Find fisher breaks for data segmentation into 5 groups.\nfish_breaks &lt;- classIntervals(world_dev_filtered$v_2015bis, n = 7, style = \"fisher\")\n\n# Assign the class breaks to the data\nworld_dev_filtered$fish_breaks &lt;- cut(world_dev_filtered$v_2015bis, fish_breaks$brks)\n\nThen we map the data:\n\nnum_bins &lt;-7\n# Define a color palette for visualizing data.\ncmap &lt;- brewer.pal(num_bins, \"YlGn\")\n\n# plot\nggplot() +\n  geom_sf(data = world_dev_filtered, aes(fill = fish_breaks)) +\ntheme_void() + # remove x and y axis\n  scale_fill_manual(\n    values = cmap,\n    name = \"GDP per capita (in 1000s)\",  # Improved legend title\n    labels = gsub(\"[,]\", \"-\", paste0(\"$\", gsub(\"[\\\\[\\\\]()]\", \" \", levels(world_dev_filtered$fish_breaks), perl = TRUE)))) +  # Replace comma with hyphen, add dollar sign, and remove brackets/parentheses from labels \n  labs(\n    title = \"GDP per capita (Quantiles)\",\n    fill = NULL  # Remove the fill label\n  ) +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nNow let’s look at the density plot\n\nfish_breaks &lt;- classIntervals(world_dev_filtered$v_2015, n = 7, style = \"fisher\")\nworld_dev_filtered$fish_breaks &lt;- cut(world_dev_filtered$v_2015, fish_breaks$brks)\n\n# Collect the values of the breaks\nfish_break_values &lt;- fish_breaks$brks\n# Place the values in a dataframe\nfish_break_values_df &lt;- data.frame(BreakValues = fish_break_values)\n\n# Create a ggplot2 visualization\nggplot(world_dev_filtered, aes(x = v_2015)) +\n# Density plot \n  geom_density(fill = \"darkblue\", alpha = 0.4) +\n# Add a rug plot (small tick marks) \n  geom_rug(alpha = 0.5) +\n# Add vertical lines at 'BreakValues' \n  geom_vline(data = fish_break_values_df, aes(xintercept = BreakValues), color = \"green\") +\n  theme_minimal() +\n  labs(x = \"GDP per capita in 2015\")\n\n\n\n\n\n\n\n\nYou will notice a lot cooler difference once you play around with a larger dataset.",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#zooming-into-the-map",
    "href": "mapvectorR.html#zooming-into-the-map",
    "title": "Lab in R",
    "section": "Zooming into the map",
    "text": "Zooming into the map\nA general map of an entire region, or urban area, can sometimes obscure local patterns because they happen at a much smaller scale that cannot be perceived in the global view. One way to solve this is by providing a focus of a smaller part of the map in a separate figure. Although there are many ways to do this in R, the most straightforward one is to define the bounding box.\nAs an example, let us consider the first ggplot map of this Lab:\n\nZoom into full map\nWe use the function coord_sf to zoom at the desired level.It’s important to know what CRS your data is in if you want to create zoomed versions of your maps. BBox finder is a useful tool to identify coordinates in EPSG:4326.\n\nggplot(data = world_dev_sf) +\n  geom_sf(aes(fill = income_group)) +\n  scale_fill_brewer(palette = \"Set4\") +  # Use ColorBrewer palette\n  theme_void() +\n  coord_sf(xlim = c(30.763478, 40.332570), ylim = c(30.520606, 36.285000)) +\n  labs(fill = \"Income Group\")  # Add a legend title\n\nWarning: Unknown palette: \"Set4\"",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  },
  {
    "objectID": "mapvectorR.html#additional-resources",
    "href": "mapvectorR.html#additional-resources",
    "title": "Lab in R",
    "section": "Additional resources",
    "text": "Additional resources\n\nOn Drawing beautiful maps with sf and ggplot see here\nIf you want to have a look at Choropleths in Python have a look at the chapter on choropleth mapping by Rey, Arribas-Bel and Wolf",
    "crumbs": [
      "3 Mapping Vector Data",
      "Lab in R"
    ]
  }
]