[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A course in Geographic Data Science",
    "section": "",
    "text": "Welcome\nThis is the website for the “Geographic Data Science” module ENVS363/563 at the University of Liverpool. This is course designed and delivered by Dr. Elisabetta Pietrostefani and Dr. Carmen Cabrera-Arnau from the Geographic Data Science Lab at the University of Liverpool, United Kingdom. Much of the course material is inspired by Dani Arribas-Bel’s course on Geographic Data Science. The previous version of this course is available here.\nThis module will introduce students to the field of Geographic Data Science (GDS), a discipline established at the intersection between Geographic Information Science (GIS) and Data Science. The course covers how the modern GIS toolkit can be integrated with Data Science tools to solve practical real-world problems.\nCore to the set of employable skills to be taught in this course is an introduction to programming tools. Students will be able to whether to develop their skills in either R or Python in Lab sessions.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "A course in Geographic Data Science",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - Module Leader - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nCarmen Cabrera-Arnau - c.cabrera-arnau [at] liverpool.ac.uk Lecturer in Geographic Data Science, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Part 1\nWeek 1 - Introduction and Open Science\nWeek 2 - Spatial Data\nWeek 3 - Mapping Vector Data\nWeek 4 - Mapping Raster Data\nWeek 5 - Points\nWeek 6 - No Lecture & Clinic\nAssignment I: Programmed Map (40% of Final Grade)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#part-2",
    "href": "syllabus.html#part-2",
    "title": "Syllabus",
    "section": "Part 2",
    "text": "Part 2\nWeek 7 - Spatial Weights\nWeek 8 - ESDA\nWeek 9 - Clustering\nWeek 10 - Spatial Network Analysis\nWeek 11 - No lecture & Clinic\nAssignment II: A computational essay (60% of Final Grade)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aims\nThe module has three main aims.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#aims",
    "href": "overview.html#aims",
    "title": "Overview",
    "section": "",
    "text": "Provide students with core competences in Geographic Data Science (GDS). This includes advancing their statistical and numerical literacy and introducing basic principles of programming and state-of-the-art computational tools for GDS;\nPresent a comprehensive overview of the main methodologies available to the Geographic Data Scientist, as well as their intuition as to how and when they can be applied;\nFocus on real world applications of these techniques in a geographical and applied context.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#learning-outcomes",
    "href": "overview.html#learning-outcomes",
    "title": "Overview",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of the module, students should be able to:\nFor all\n\nDemonstrate advanced GIS/GDS concepts and be able to use the tools programmatically to import, manipulate and analyse data in different formats.\nUnderstand the motivation and inner workings of the main methodological approaches of GDS, both analytical and visual.\nEvaluate the suitability of a specific technique, what it can offer and how it can help answer questions of interest.\nApply a number of spatial analysis techniques and how to interpret the results, in the process of turning data into information.\nWhen faced with a new data-set, work independently using GIS/GDS tools programmatically.\n\nOnly for MSc students\n\nDemonstrate a sound understanding of how real-world (geo)data are produced, their potential insights and biases, as well as opportunities and limitations.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "overview.html#feedback",
    "href": "overview.html#feedback",
    "title": "Overview",
    "section": "Feedback",
    "text": "Feedback\nFormal assessment of one map and one computational essays. Written assignment-specific feedback will be provided within three working weeks of the submission deadline. Comments will offer an understanding of the mark awarded and identify areas which can be considered for improvement in future assignments.\nVerbal face-to-face feedback. Immediate face-to-face feedback will be provided during computer, discussion and clinic sessions in interaction with staff. This will take place in all live sessions during the semester.\nOnline forum. Asynchronous written feedback will be provided via an online forum. Students are encouraged to contribute by asking and answering questions relating to the module content. Staff will monitor the forum Monday to Friday 9am-5pm, but it will be open to students to make contributions at all times. Response time will vary depending on the complexity of the question and staff availability.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "assess.html",
    "href": "assess.html",
    "title": "Assessments",
    "section": "",
    "text": "Assignment I\nThis assignment will be evaluated on technical data processing, map design abilities (assemblage), and design overall narrative.\nOnce you have created your map, you will need to present it. Write up to 500 about the choices you made to create the map.\nYou will submit an .html file obtained by rendering your .qmd in R or .ipynb Jupyter Notebook in Python.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#assignment-i",
    "href": "assess.html#assignment-i",
    "title": "Assessments",
    "section": "",
    "text": "Title: Programmed Map\nType: Coursework\nDue date: 30th October 2025 (week 6)\n40% of the final mark\nChance to be reassessed\nElectronic submission only\n\n\n\n\nFull Assignment details to come\n\n\n\nIf you are doing the assignment in R: You can start from this .qmd file to render the html. Other file formats will not be accepted.\nIf you are doing the assignment in Python: To do so, in your .ipynb file, follow these steps: File –&gt; Save and Export as.. –&gt; HTML. Prior to this step, the notebook needs to be rendered (i.e. all the cells should be executed). Other file formats will not be accepted.\n\n\nSubmit\nOnce completed, you will need to submit the following:\nAn html version of an .qmd document with R integrated code.\nThe assignment will be evaluated based on three main pillars, on which you will have to be successful to achieve a good mark:\n\nData Processing: Your proficiency in handling and manipulating data will be a fundamental aspect of the assessment.\nMap assemblage This includes your ability to master technologies that allow you to create a compelling map.\nDesign and narrative: Your success in designing an appealing map with a compelling narrative will play a pivotal role in your overall evaluation.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#assignment-ii",
    "href": "assess.html#assignment-ii",
    "title": "Assessments",
    "section": "Assignment II",
    "text": "Assignment II\n\nTitle: Computational Essay\nType: Coursework\nDue date: 4th December 2025 (week 11)\n60% of the final mark\nChance to be reassessed\nElectronic submission only\n\nA 4,000 word computational essay on a geographic data set which they have explored and analysed using the skills and techniques developed during the course. Students will complete an essay which combines both code, data visualisation and prose supported by references in order to demonstrate sound understanding of all learning outcomes.\n\nFull Assignment details to come\n\nImportant information about data access through the US Census API:\n\nIn R: ENVS2425-363-563.2.qmd\nIn Python: ENVS2425-363-563.2.ipynb\n\nOverview\nHere’s the premise. You will take the role of a real-world geographic data scientist tasked to explore datasets on Los Angeles and find useful insights for a variety of city decision-makers. It does not matter if you have never been to the Los Angeles. In fact, this will help you focus on what you can learn about the city through the data, without the influence of prior knowledge. Furthermore, the assessment will not be marked based on how much you know about Los Angeles but instead about how much you can show you have learned through analysing data. You will need contextualise your project by highlighting the opportunities and limitations of ‘old’ and ‘new’ forms of spatial data and reference relevant literature.\nWhat is a Computational Essay?\nA computational essay is an essay whose narrative is supported by code and computational results that are included in the essay itself. This piece of assessment is equivalent to 4,000 word. However, this is the overall weight. Since you will need to create not only narrative but also code and figures, here are the requirements:\n\nMaximum of 1,000 words of ordinary text (references do not contribute to the word count). You should answer the specified questions within the narrative. The questions should be included within a wider analysis.\nUp to four maps or figures (a figure may include more than one map and will only count as one but needs to be integrated in the same overall output)\nUp to one table\n\nThere are three kinds of elements in a computational essay:\n\nOrdinary text (in English)\nComputer input (R or Python)\nComputer output\n\nThese three elements all work together to express what’s being communicated.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#marking-criteria",
    "href": "assess.html#marking-criteria",
    "title": "Assessments",
    "section": "Marking Criteria",
    "text": "Marking Criteria\nThis course follows the standard marking criteria (the general ones and those relating to GIS assignments in particular) set by the School of Environmental Sciences. Please make sure to check the student handbook and familiarise with them. In addition to these generic criteria, the following specific criteria will be used in cases where computer code is part of the work being assessed:\n\n0-15: the code does not run and there is no documentation to follow it.\n16-39: the code does not run, or runs but it does not produce the expected outcome. There is some documentation explaining its logic.\n40-49: the code runs and produces the expected output. There is some documentation explaining its logic.\n50-59: the code runs and produces the expected output. There is extensive documentation explaining its logic.\n60-69: the code runs and produces the expected output. There is extensive documentation, properly formatted, explaining its logic.\n70-79: all as above, plus the code design includes clear evidence of skills presented in advanced sections of the course (e.g. custom methods, list comprehensions, etc.).\n80-100: all as above, plus the code contains novel contributions that extend/improve the functionality the student was provided with (e.g. algorithm optimizations, novel methods to perform the task, etc.).",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "assess.html#generative-artificial-intelligence",
    "href": "assess.html#generative-artificial-intelligence",
    "title": "Assessments",
    "section": "Generative Artificial Intelligence",
    "text": "Generative Artificial Intelligence\n• You are reminded that the inappropriate use of Generative Artificial Intelligence Tools in the preparation of assignments is strictly prohibited.\n• Assignments should be prepared using your own words. All use of AI translation tools should be properly acknowledged. Extensive use of AI proof-reading tools is prohibited. Whilst you may use spelling/grammar checks typically found in word-processing packages, using AI tools to change words/sentence structure may incur an Academic Integrity penalty.\n• If your assessment is referred for an Academic Integrity Investigation, you may be asked to demonstrate that the work you have submitted is your own. Therefore, it is advised that you keep hold of earlier files, drafts, notes and other relevant preparatory materials that you have used.",
    "crumbs": [
      "Assessments"
    ]
  },
  {
    "objectID": "environ.html",
    "href": "environ.html",
    "title": "Environment",
    "section": "",
    "text": "Coding Languages\nThis course can be followed by anyone with access to a bit of technical infrastructure. This section details the set of local and online requirements you will need to be able to follow along, as well as instructions or pointers to get set up on your own. This is a centralised section that lists everything you will require.\nIn this course, you have the option to follow along using either R or Python, depending on your past experience with these programming languages and preference. Please choose one language to focus on and stick to it throughout.\nThis course has two assignments and you will be required to submit both assignments in the same programming languages. The next two sections will guide you through the process of setting up your development environment in R or Python, so you can get started with the course smoothly.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environ.html#coding-languages",
    "href": "environ.html#coding-languages",
    "title": "Environment",
    "section": "",
    "text": "If you want to follow the course in R, you can find instructions to set up your environment here.\nIf you want to follow the course in Python, you can find instructions to set up your environment here.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environ.html#reproducing-code-in-this-book",
    "href": "environ.html#reproducing-code-in-this-book",
    "title": "Environment",
    "section": "Reproducing code in this book",
    "text": "Reproducing code in this book\nIf you want to reproduce the code in the book, you need the most recent version of Quarto, R and relevant packages. These can be installed following the instructions provided in our R installation guide. Quarto (1.2.280) can be downloaded from the Quarto website, it may already be installed when you download R and R Studio.",
    "crumbs": [
      "Environment"
    ]
  },
  {
    "objectID": "environR.html",
    "href": "environR.html",
    "title": "R",
    "section": "",
    "text": "R Basics\nTo run the analysis and reproduce the code in R, you need the following software:\nTo install and update:\nTo check your version of:",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#r-basics",
    "href": "environR.html#r-basics",
    "title": "R",
    "section": "",
    "text": "Starting a session\nUpon startup, RStudio will look something like this. Note: the Pane Layout and Appearance settings can be altered e.g. on Mac OS by clicking RStudio&gt;Preferences&gt;Appearance and RStudio&gt;Preferences&gt;Pane Layout. I personally like to have my Console in the top right corner and Environment in the bottom left and keep the Source and Environment panes wider than Console and Files for easier readability. Default settings will probably have the Console in the bottom left and Environment in the top right. You will also have a standard white background; but you can chose specific themes.\n\n\n\n\n\n\n\n\n\nAt the start of a session, it’s good practice clearing your R environment:\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set. You should have set this following the pre-recorded video.\n\ngetwd() \n\nIf the directory is not set yet, type in setwd(\"~/pathtodirectory\") to set it. It is crucial to perform this step at the beginning of your R script, so that relative paths can be used in the subsequent parts.\n\nsetwd(\"~/Dropbox/Github/gds\")\n\nIf you have set your directory correctly, you can check it with getwd()\n\n\n\n\n\n\n\n\n\nImportant: You do not need to set your working directory if you are using an R-markdown or Quarto document and you have it saved in the right location. The pathway will start from where your document is saved.\n\n\nUsing the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\nR Objects\nEverything in R is an object. R possesses a simple generic function mechanism which can be used for an object-oriented style of programming. Indeed, everything that happens in R is the result of a function call (John M. Chambers). Method dispatch takes place based on the class of the first argument to the generic function.\nAll R statements where you create objects – “assignments” – have this form: object_name &lt;- value. Assignment can also be performed using = instead of &lt;-, but the standard advice is to use the latter syntax (see e.g. The R Inferno, ch. 8.2.26). In RStudio, the standard shortcut for the assignment operator &lt;- is Alt + - (in Windows) or option + - (in Mac OS).\nA mock assignment of the value 30 to the name age is reported below. In order to inspect the content of the newly created variable, it is sufficient to type the name into the console. Within R, the hash symbol # is used to write comments and create collapsible code sections.\n\nage &lt;- 30 # Assign the number 30 to the name \"age\"\nage # print the variable \"age\" to the console\n\n[1] 30\n\n\n\n\nA small note on variable types\nThe function class() is used to inspect the type of an object.\nThere are four main types of variables:\n\nLogical: boolean/binary, can either be TRUE or FALSE\n\n\nclass(TRUE)\n\n[1] \"logical\"\n\n\n\nCharacter (or string): simple text, including symbols and numbers. It can be wrapped in single or double quotation, which usually highlights text in a different colour in RStudio\n\n\nclass(\"I am a city\")\n\n[1] \"character\"\n\n\n\nNumeric: Numbers. Mathematical operators can be used here.\n\n\nclass(2022)\n\n[1] \"numeric\"\n\n\n\nFactor: Characters or strings, but ordered in categories.\n\n\nclass(as.factor(c(\"I\", \"am\", \"a\", \"factor\")))\n\n[1] \"factor\"\n\n\nAnother important value to know is NA. It stands for “Not Available” and simply denotes a missing value.\n\nvector_with_missing &lt;- c(NA, 1, 2, NA)\nvector_with_missing\n\n[1] NA  1  2 NA\n\n\n\n\nLogical operators and expressions\n\n== asks whether two values are the same or equal (“is equal to”)\n!= asks whether two values are the not the same or unequal (“is not equal to”)\n&gt; greater than\n&gt;= greater or equal to\n&lt;= smaller or equal to\n& stands for “and” (unsurprisingly)\n| stands for “or”\n! stands for “not",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#installing-packages",
    "href": "environR.html#installing-packages",
    "title": "R",
    "section": "Installing packages",
    "text": "Installing packages\nIn R, packages are collections of functions, compiled code and sample data. They functionally act as “extensions” to the base R language, and can help you accomplish all operations you might want to perform in R (if no package serves your purpose, you may want to write an entirely new one!). Now, we will install the R package tidyverse. Look at the link to see what tidyverse includes, and directly load a .csv file (comma-separated values) into R from your computer.\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#examples",
    "href": "environR.html#examples",
    "title": "R",
    "section": "Examples",
    "text": "Examples\nLet’s create some random R objects:\n\n## Entering random \nLondon  &lt;- 8982000 # population\nBristol &lt;- 467099 # population\nLondon_area &lt;-1572 # area km2\nBristol_area &lt;-110 # area km2\n\nLondon\n\n[1] 8982000\n\n\nCalculate Population Density in London:\n\nLondon_pop_dens &lt;- London/London_area\nBristol_pop_dens &lt;- Bristol/Bristol_area\n\nLondon_pop_dens\n\n[1] 5713.74\n\n\nThe function c(), which you will use extensively if you keep coding in R, means “concatenate”. In this case, we use it to create a vector of population densities for London and Bristol:\n\nc(London_pop_dens, Bristol_pop_dens)\n\n[1] 5713.740 4246.355\n\npop_density &lt;- c(London_pop_dens, Bristol_pop_dens) # In order to create a vector in R we make use of c() (which stands for concatenate)\n\nCreate a character variable:\n\nx &lt;- \"a city\"\nclass(x)\n\n[1] \"character\"\n\ntypeof(x)\n\n[1] \"character\"\n\nlength(x)\n\n[1] 1",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#data-structures",
    "href": "environR.html#data-structures",
    "title": "R",
    "section": "Data Structures",
    "text": "Data Structures\nObjects in R are typically stored in data structures. There are multiple types of data structures:\n\nVectors\nIn R, a vector is a sequence of elements which share the same data type. A vector supports logical, integer, double, character, complex, or raw data types.\n\n# first vector y\ny &lt;- 1:10\nas.numeric(y)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nclass(y)\n\n[1] \"integer\"\n\nlength(y)\n\n[1] 10\n\n# another vector z\nz &lt;- c(2, 4, 56, 4)\nz\n\n[1]  2  4 56  4\n\n# and another one called cities\ncities &lt;- c(\"London\", \"Bristol\", \"Bath\")\ncities\n\n[1] \"London\"  \"Bristol\" \"Bath\"   \n\n\n\n\nMatrices\nTwo-dimensional, rectangular, and homogeneous data structures. They are similar to vectors, with the additional attribute of having two dimensions: the number of rows and columns.\n\nm &lt;- matrix(nrow = 2, ncol = 2)\nm\n\n     [,1] [,2]\n[1,]   NA   NA\n[2,]   NA   NA\n\nn &lt;- matrix(c(4, 5, 78, 56), nrow = 2, ncol = 2 )\nn\n\n     [,1] [,2]\n[1,]    4   78\n[2,]    5   56\n\n\n\n\nLists\nLists are containers which can store elements of different types and sizes. A list can contain vectors, matrices, dataframes, another list, functions which can be accessed, unlisted, and assigned to other objects.\n\nlist_data &lt;- list(\"Red\", \"Green\", c(21,32,11), TRUE, 51.23, 119.1)\nprint(list_data)\n\n[[1]]\n[1] \"Red\"\n\n[[2]]\n[1] \"Green\"\n\n[[3]]\n[1] 21 32 11\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] 51.23\n\n[[6]]\n[1] 119.1\n\n\n\n\nData frames\nThey are the most common way of storing data in R and are the most used data structure for statistical analysis. Data frames are “rectangular lists”, i.e. tabular structures in which every element has the same length, and can also be thought of as lists of equal length vectors.\n\n## Here is a data frame of 3 columns named id, x, y and 10 rows\ndat &lt;- data.frame(id = letters[1:10], x = 1:10, y = 11:20)\ndat\n\n   id  x  y\n1   a  1 11\n2   b  2 12\n3   c  3 13\n4   d  4 14\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nhead(dat) # read first 5 rows\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ntail(dat)\n\n   id  x  y\n5   e  5 15\n6   f  6 16\n7   g  7 17\n8   h  8 18\n9   i  9 19\n10  j 10 20\n\nnames(dat)\n\n[1] \"id\" \"x\"  \"y\" \n\n\nDataframes in R are indexed by rows and columns numbers using the [rows,cols] syntax. The $ operator allows you to access columns in the dataframe, or to create new columns in the dataframe.\n\ndat[1,] # read first row and all colum ns\n\n  id x  y\n1  a 1 11\n\ndat[,1] # read all rows and the first column\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\ndat[6,3] # read 6th row, third column\n\n[1] 16\n\ndat[c(2:4),] # read rows 2 to 4 and all columns\n\n  id x  y\n2  b 2 12\n3  c 3 13\n4  d 4 14\n\ndat$y # read column y\n\n [1] 11 12 13 14 15 16 17 18 19 20\n\ndat[dat$x&lt;7,] # read rows that have a x value less than 7\n\n  id x  y\n1  a 1 11\n2  b 2 12\n3  c 3 13\n4  d 4 14\n5  e 5 15\n6  f 6 16\n\ndat$new_column &lt;- runif(10, 0, 1) # create a new variable called \"new_column\"\n\ndat\n\n   id  x  y new_column\n1   a  1 11 0.12057209\n2   b  2 12 0.16461165\n3   c  3 13 0.10111758\n4   d  4 14 0.16387671\n5   e  5 15 0.57811083\n6   f  6 16 0.06151680\n7   g  7 17 0.70395518\n8   h  8 18 0.04066187\n9   i  9 19 0.33875033\n10  j 10 20 0.66096785",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#exercises",
    "href": "environR.html#exercises",
    "title": "R",
    "section": "Exercises",
    "text": "Exercises\n\n1. Vectors\n\nAssign the first 10 elements of the Fibonacci sequence to a numeric vector called fibonacci_vector.\n\n\n\nShow the code\nfibonacci_vector &lt;- c(0, 1, 1, 2, 3, 5, 8, 13, 21, 34)\n\n\n\nAssign the names of the people sitting at your table to a character vector.\n\n\n\nShow the code\npeople_vector &lt;- c(\"Elisabetta\", \"Carmen\", \"Habib\")\n\n\n\nInspect the length and class of your numeric and character vectors.\n\n\n\nShow the code\nlength(fibonacci_vector)\n\n\n[1] 10\n\n\nShow the code\nlength(people_vector)\n\n\n[1] 3\n\n\nShow the code\nclass(fibonacci_vector)\n\n\n[1] \"numeric\"\n\n\nShow the code\nclass(people_vector)\n\n\n[1] \"character\"\n\n\n\nConstruct a numeric vector containing 10 numbers generated at random from the Uniform distribution with interval [0,1] (Hint: runif()).\n\n\n\nShow the code\nrandom_uniform &lt;- runif(10, 0, 1)\n\n\n\nMultiply this vector by a scalar.\n\n\n\nShow the code\nrandom_uniform*3\n\n\n [1] 2.5494715 1.3127550 2.4584536 0.6895074 2.2425527 1.3976348 0.7799749\n [8] 1.4578196 0.2121431 2.2033858\n\n\n\nConstruct a numeric vector by multiplying fibonacci_vector by the vector constructed at step 4.\n\n\n\nShow the code\nnew_numeric_vector &lt;- fibonacci_vector*random_uniform\n\n\n\n\n2. Matrices\n\nConstruct a 3x3 matrix containing fibonacci_vector, the vector of random draws from the uniform distribution, and their multiplication.\n\n\n\nShow the code\nnew_matrix &lt;-  matrix(c(fibonacci_vector, random_uniform, new_numeric_vector), ncol =3)\n\n\n\nConvert the matrix to a dataframe (Hint: as.data.frame())\n\n\n\nShow the code\nnew_df &lt;-  as.data.frame(new_matrix)\n\n\n\nName the dataframe columns (Hint: dplyr::rename())\n\n\n\nShow the code\nnew_df &lt;-  new_df %&gt;%\n  dplyr::rename(fibonacci_vector = V1,\n                random_uniform = V2,\n                new_numeric_vector = V3)\n\n\n\n\n3. Data Frames\n\nConstruct a Data Frame with 5 columns with an ID, City Name, Population, Area and Population density of 3 cities in the UK. You can use London, Bristol and other cities in the UK.\n\n\n\nShow the code\nUK_cities = data.frame(\n  id = c(1,2,3),\n  city_name = c(\"London\", \"Bristol\", \"Liverpool\"),\n  population = c(8982000, 467099, 864122),\n  area = c(1572, 110, 200)\n)\n\nUK_cities$pop_density = UK_cities$population/UK_cities$area\n\n# or the tidy way\nUK_cities_tidy = UK_cities %&gt;%\n  mutate(pop_density = population/area)\n\n# Get the structure of the data frame\nstr(UK_cities)\n\n\n'data.frame':   3 obs. of  5 variables:\n $ id         : num  1 2 3\n $ city_name  : chr  \"London\" \"Bristol\" \"Liverpool\"\n $ population : num  8982000 467099 864122\n $ area       : num  1572 110 200\n $ pop_density: num  5714 4246 4321\n\n\nShow the code\n# Print the summary\nprint(summary(UK_cities))\n\n\n       id       city_name           population           area       \n Min.   :1.0   Length:3           Min.   : 467099   Min.   : 110.0  \n 1st Qu.:1.5   Class :character   1st Qu.: 665610   1st Qu.: 155.0  \n Median :2.0   Mode  :character   Median : 864122   Median : 200.0  \n Mean   :2.0                      Mean   :3437740   Mean   : 627.3  \n 3rd Qu.:2.5                      3rd Qu.:4923061   3rd Qu.: 886.0  \n Max.   :3.0                      Max.   :8982000   Max.   :1572.0  \n  pop_density  \n Min.   :4246  \n 1st Qu.:4283  \n Median :4321  \n Mean   :4760  \n 3rd Qu.:5017  \n Max.   :5714",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#import-data-from-csv",
    "href": "environR.html#import-data-from-csv",
    "title": "R",
    "section": "Import data from csv",
    "text": "Import data from csv\n\nDensities_UK_cities &lt;- read_csv(\"data/London/Tables/Densities_UK_cities.csv\")\n\nRows: 76 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): city, pop\ndbl (1): n\nnum (2): area, density\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nDensities_UK_cities\n\n# A tibble: 76 × 5\n       n city               pop        area density\n   &lt;dbl&gt; &lt;chr&gt;              &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1     1 Greater London     9,787,426 1738.    5630\n 2     2 Greater Manchester 2,553,379  630.    4051\n 3     3 West Midlands      2,440,986  599.    4076\n 4     4 West Yorkshire     1,777,934  488.    3645\n 5     5 Greater Glasgow    957,620    368.    3390\n 6     6 Liverpool          864,122    200.    4329\n 7     7 South Hampshire    855,569    192     4455\n 8     8 Tyneside           774,891    180.    4292\n 9     9 Nottingham         729,977    176.    4139\n10    10 Sheffield          685,368    168.    4092\n# ℹ 66 more rows\n\n\nYou can also view the data set with:\n\nglimpse(Densities_UK_cities)\n\nRows: 76\nColumns: 5\n$ n       &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,…\n$ city    &lt;chr&gt; \"Greater London\", \"Greater Manchester\", \"West Midlands\", \"West…\n$ pop     &lt;chr&gt; \"9,787,426\", \"2,553,379\", \"2,440,986\", \"1,777,934\", \"957,620\",…\n$ area    &lt;dbl&gt; 1737.9, 630.3, 598.9, 487.8, 368.5, 199.6, 192.0, 180.5, 176.4…\n$ density &lt;dbl&gt; 5630, 4051, 4076, 3645, 3390, 4329, 4455, 4292, 4139, 4092, 42…\n\ntable(Densities_UK_cities$city)\n\n\n               Aberdeen  Accrington/ Rossendale Barnsley/ Dearne Valley \n                      1                       1                       1 \n               Basildon             Basingstoke                 Bedford \n                      1                       1                       1 \n                Belfast              Birkenhead               Blackburn \n                      1                       1                       1 \n              Blackpool      Bournemouth/ Poole       Brighton and Hove \n                      1                       1                       1 \n                Bristol                 Burnley       Burton-upon-Trent \n                      1                       1                       1 \n              Cambridge                 Cardiff              Chelmsford \n                      1                       1                       1 \n             Cheltenham            Chesterfield              Colchester \n                      1                       1                       1 \n               Coventry                 Crawley                   Derby \n                      1                       1                       1 \n              Doncaster                  Dundee              Eastbourne \n                      1                       1                       1 \n              Edinburgh                  Exeter  Farnborough/ Aldershot \n                      1                       1                       1 \n             Gloucester         Greater Glasgow          Greater London \n                      1                       1                       1 \n     Greater Manchester                 Grimsby                Hastings \n                      1                       1                       1 \n           High Wycombe                 Ipswich      Kingston upon Hull \n                      1                       1                       1 \n              Leicester                 Lincoln               Liverpool \n                      1                       1                       1 \n                  Luton               Maidstone               Mansfield \n                      1                       1                       1 \n           Medway Towns           Milton Keynes              Motherwell \n                      1                       1                       1 \n                Newport             Northampton                 Norwich \n                      1                       1                       1 \n             Nottingham                  Oxford       Paignton/ Torquay \n                      1                       1                       1 \n           Peterborough                Plymouth                 Preston \n                      1                       1                       1 \n                Reading               Sheffield                  Slough \n                      1                       1                       1 \n        South Hampshire         Southend-on-Sea          Stoke-on-Trent \n                      1                       1                       1 \n             Sunderland                 Swansea                 Swindon \n                      1                       1                       1 \n               Teesside                 Telford                  Thanet \n                      1                       1                       1 \n               Tyneside              Warrington           West Midlands \n                      1                       1                       1 \n         West Yorkshire                   Wigan               Worcester \n                      1                       1                       1 \n                   York \n                      1",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#r-list-of-libraries",
    "href": "environR.html#r-list-of-libraries",
    "title": "R",
    "section": "R List of libraries",
    "text": "R List of libraries\nThe list of libraries used in this book is provided below:\n\ntidyverse\ndata.table\nsf\ntmap\nreadr\ngeojsonsf\nosmdata\nbasemapR\nRColorBrewer\nclassInt\nR.utils\ndplyr\nggplot2\nviridis\nraster\nterra\nexactextractr\ntidyterra\nspdep\ntibble\npatchwork\nrosm\ntidyr\nGGally\ncluster\nrgeoda\nmapview\nggspatial\ncolorspace\ngstat\nspatstat\ndbscan\nfpc\neks\nigraph\ntidygraph\n\nYou need to ensure you have installed the list of libraries used in this book, running the following code:\n\n# package names\npackages &lt;- c(\n  \"tmap\", \"readr\", \"geojsonsf\", \"osmdata\", \"basemapR\", \"sf\", \"tidyverse\", \n  \"RColorBrewer\", \"classInt\", \"R.utils\", \"dplyr\", \"ggplot2\", \"viridis\", \n  \"raster\", \"terra\", \"exactextractr\", \"tidyterra\", \"spdep\", \"tibble\", \n  \"patchwork\", \"rosm\", \"tidyr\", \"GGally\", \"cluster\", \"rgeoda\", \"mapview\", \n  \"ggspatial\", \"colorspace\", \"gstat\", \"spatstat\", \"dbscan\", \"fpc\", \"eks\", \n  \"igraph\", \"tidygraph\"\n)\n\n# Function to check and install missing packages\ninstall_if_missing &lt;- function(p) {\n  if (!requireNamespace(p, quietly = TRUE)) {\n    install.packages(p)\n  }\n}\n\n# Apply the function to each package\ninvisible(sapply(packages, install_if_missing))",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environR.html#resources",
    "href": "environR.html#resources",
    "title": "R",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nR for Data Science. R4DS teaches you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it.\nSpatial Data Science by Edzer Pebesma and Roger Bivand introduces and explains the concepts underlying spatial data.\nGeo-computation with R by Robin Lovelace, Jakub Nowosad and Jannes Muenchow.",
    "crumbs": [
      "Environment",
      "R"
    ]
  },
  {
    "objectID": "environPy.html",
    "href": "environPy.html",
    "title": "Python",
    "section": "",
    "text": "Set up Miniconda (and Python) on Ms Windows\nTo run the analysis and reproduce the code in Python, you will need to set up the Python environment to:\nWe will use Miniconda to handle our working environment.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#set-up-miniconda-and-python-on-ms-windows",
    "href": "environPy.html#set-up-miniconda-and-python-on-ms-windows",
    "title": "Python",
    "section": "",
    "text": "Installation\n\nInstall Miniconda:\n\nOption 1: On a UoL Machine: Anaconda is installed on many university machines. Please check whether it is installed. If not, download and install Anaconda through from Install University Applications, type and choose Anaconda.\nOption 2: Recommended - Install Miniconda on your personal Laptop: Follow the instructions here.\n\nDuring the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\n\n\n\n\n\nUniversity Machines\n\n\n\nIf you do choose to work on University Machines\n\nChose a machine where Anaconda has been pre-installed.\nAlways use the same machine. For example if on the first day you are using CT60 Station 17 - Orange Zone, continue using this machine for the rest of the course. If you change machine you will need to re-install the environment every time.\n\n\n\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it envs363_563. You can save it wherever you want. If you are working on a university machine, it could be worth creating it in M:/, which is your “virtual” hard-disk.\n\n\n\nDownload the data to run and render the jupyter notebooks. To learn how to download folders from github see here.\nUnzip the folders and move the nested folders into the folder envs363_563.\nCreate another folder called labs\n\nThe folder structure should look like:\nenvs363_563/\n├── data/\n└── labs/\n\n\nSet up the Python Environment\n\nDownload the envs363_563.yml from GitHub by cliciking Download raw file, top right at this page\nSave it in the folder envs363_563 created before.\nType in the search bar and find the Anaconda Powershell Prompt if working on University Machine or Anaconda Prompt (miniconda 3) if on your personal. Launch it. The terminal should appear.\n\n\n\nIn the Anaconda Terminal write: conda env create -n envs363_563 --file M:\\envs363\\envs363_563.yml and press Enter; if the file is located elsewhere you’ll need to use the corresponding file path.\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nIn the Anaconda Terminal write conda activate envs363_563 and press Enter. This activates your working environment.\n\n\n\n\nNecessary on University machines, otherwise Optional: Configuration of Jupyter Notebooks\n\nIn the Anaconda Terminal, write jupyter server --generate-config and press enter. This, at least in Windows, should create a file to: C:\\Users\\username\\.jupyter\\jupyter_server_config.py.\nOpen the file with a text editor (e.g. Notepad++), do a ctrl-f search for: c.ServerApp.root_dir, uncomment it by removing the # and change it to c.ServerApp.notebook_dir = 'M:\\\\your\\\\new\\\\path, for example the directory where you created the envs363_563 folder. In the University Machines, it is advised to work on the directory M:\\.\nSave the file and close it.\n\n\n\n\n\nStart a Lab Session\n\nDownload the Jupyter Notebook of the session in your folder. Choose one jupyter notebook and click Dowload raw file as shown below.\n\n\n\nSave the file in the labs folder within your envs363_563 folder on your machine.\nType in the search bar, find and open the Anaconda Prompt (miniconda 3).\nIn the Anaconda Terminal write and run conda activate envs363_563.\nIn the Anaconda Terminal write and run jupyter notebook. This should open Jupyter Notebook in your default browser.\n\n\n\nNavigate to your course folder in and double click on the notebook downloaded in step 1.\nYou can now work on your copy of the notebook.\n\nFollow these instructions and test your installation prior to the first Lab Session. If you experience any issues, write a message on the Ms Teams channel of the module. Setting up the Python environment is necessary for:\n\nExecuting the Jupyter Notebooks of the Lab sessions of the course.\nPreparing your own Jupyter Notebooks for the assignments (one each).",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#set-up-miniconda-and-python-on-mac",
    "href": "environPy.html#set-up-miniconda-and-python-on-mac",
    "title": "Python",
    "section": "Set up Miniconda (and Python) on MAC",
    "text": "Set up Miniconda (and Python) on MAC\n\nInstallation\nTo install Miniconda on your personal laptop, Follow the instructions here. During the installation, leave the default settings. In particular, when asked whom to “Install Miniconda for”, choose “Just for me”.\n\n\nSet up the Directories\n\nCreate a folder where you want to keep your work conducted throughout this course. For example, call it envs363_563. You can save it wherever you want. For example, Elisabetta has named her folder envs363_563 and it’s in her Dropbox in Users/PIETROST/Library/CloudStorage/Dropbox/envs363_563\nDownload the data to run and render the jupyter notebooks. To learn how to download folders from github see here.\nUnzip the folders and move the nested folders into the folder envs363_563.\nCreate another folder called labs\n\nThe folder structure should look like:\nenvs363_563/ ├── data/ └── labs/\n\n\n\nSet up the Python Environment\n\nDownload the envs363_563.yml from GitHub by clicking Download raw file, top right at this page\nSave it in the folder envs363_563 created before.\nType in the search bar and open the Terminal.\nIn the Terminal write conda env create -n envs363 --file envs363_563.yml and press Enter. This will need to be modified according to where you placed the envs363_563 folder. For example, Elisabetta has named her folder envs363_563 and it’s in her Dropbox in Users/PIETROST/Library/CloudStorage/Dropbox/envs363_563/envs363_563.yml. If you created the envs363_563 folder on your desktop, the path would be Desktop/envs363_563.\n\n\n\nIf you are prompted any questions, press y. This process will install all the packages necessary to carry out the lab sessions.\nYou should then see this\n\n\n\n\nStart a Lab Session\n\nDownload the Jupyter Notebook of the session in your folder. Choose one jupyter notebook and click Dowload raw file as shown below\n\n\n\nSave the file in the labs folder within your envs363 folder on your machine.\nType in the search bar, find and open the Terminal.\nIn the Terminal write and run conda activate envs363.\nIn the Terminal write and run jupyter notebook.\n\n\n\nThis should open Jupyter Notebook in your default browser. You should see something like this:\n\n\n\nNavigate to your folder. You can now work on your copy of the notebook.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#py-basics",
    "href": "environPy.html#py-basics",
    "title": "Python",
    "section": "Py Basics",
    "text": "Py Basics\nPlease refer to the tutorials from learnpython.org for an introduction to coding in Python. We particularly recommend the tutorials listed under the “Learn the Basics” section.",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "environPy.html#resources",
    "href": "environPy.html#resources",
    "title": "Python",
    "section": "Resources",
    "text": "Resources\nSome help along the way with:\n\nGeographic Data Science with Python.\nPython for Geographic Data Analysis",
    "crumbs": [
      "Environment",
      "Python"
    ]
  },
  {
    "objectID": "download.html",
    "href": "download.html",
    "title": "Download data from github",
    "section": "",
    "text": "Go to https://download-directory.github.io\n\n\n\nGo to the folder you need for your Lab. For example copy: https://github.com/pietrostefani/gds/tree/main/data/London\n\n\n\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "Environment",
      "Download data from github"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "From Geographic Information Systems to Geographic Data Science\nIn this section we introduce what Geographic Data Science is. We top it up with a few (optional) further readings for the interested and curious mind.\nSlides can be downloaded “here”\nGeographic Information holds a pivotal position within our modern societies, permeating various aspects of our daily lives. It underpins essential sectors such as housing, transportation, insurance, banking, telecommunications, logistics, energy, retail, agriculture, healthcare, and urban planning. Its significance lies in the capacity to analyze and derive invaluable insights from geo-spatial data, enabling us to make informed decisions and address complex challenges. Proficiency in this field equips individuals with the ability to work with real-world data across multiple domains and tackle diverse problems. Furthermore, it provides the opportunity to acquire essential data science skills and utilize important tools for answering spatial questions. Given its wide-ranging applications and the increasing reliance on location-based information, there is a substantial demand for experts in the geographic information industry, making it a highly sought-after skill set in today’s workforce.\nWhat information does GIS use?\nFrom the real-world to the GIS world\nGeographic Data Science\nA GIS person typically produces cartographic and analytical products using desktop software. A geospatial data scientist creates code and runs pipelines that produce analytical products and cartographic representations.\nThis entails working with real-world data from various domains and tackling a wide range of complex problems. Through this process geospatial data science includes both data science and GIS tools that lead to the analysos of intricate spatial questions effectively. The synergy between CyberGIS and Geographic Data Science is unmistakable, with coding playing a pivotal role in enabling the seamless development of interactive data analysis. By leveraging cutting-edge technologies and innovative methodologies, this symbiotic relationship enhances the accessibility, scalability, and interactivity of geospatial data analysis. Consequently, it opens up new vistas for collaborative research and decision-making processes.\nThis multifaceted approach equips them with the knowledge and expertise to navigate the intricate world of spatial data analysis and contribute meaningfully to diverse fields where location-based insights are invaluable.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#from-geographic-information-systems-to-geographic-data-science",
    "href": "intro.html#from-geographic-information-systems-to-geographic-data-science",
    "title": "1 Introduction",
    "section": "",
    "text": "Data that defines geographical features like roads, rivers\nSoil types, land use, elevation\nDemographics, socioeconomic attributes\nEnvironmental, climate, air-quality\nAnnotations that label features and places",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#what-is-geographic-data-science",
    "href": "intro.html#what-is-geographic-data-science",
    "title": "1 Introduction",
    "section": "What is Geographic Data Science?",
    "text": "What is Geographic Data Science?\nStatistician George Box :\nAll models are wrong, but some are useful In a similar fashion.\nGeographer Keith Ord :\nAll maps are wrong, but some are useful.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#open-source-gis",
    "href": "intro.html#open-source-gis",
    "title": "1 Introduction",
    "section": "Open Source GIS",
    "text": "Open Source GIS\nOpen source Geographic Information Systems (GIS), such as QGIS, have made geographic analysis accessible worldwide. GIS programs tend to emphasize graphical user interfaces (GUIs), with the unintended consequence of discouraging reproducibility (although many can be used from the command line Python + QGIS). R and Python by contrast, emphasizes the command line interface (CLI).\nThe ‘geodata revolution’ drives demand for high performance computer hardware and efficient, scalable software to handle and extract signal from the noise, to understand and perhaps change the world. Spatial databases enable storage and generation of manageable subsets from the vast geographic data stores, making interfaces for gaining knowledge from them vital tools for the future.\nR and Python are both tools with advanced modeling and visualization capabilities.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#open-science",
    "href": "intro.html#open-science",
    "title": "1 Introduction",
    "section": "Open Science",
    "text": "Open Science\nWhy do we care about the processes and tools we use when we do computational work? Where do the current paradigm come from? Are we on the verge of a new model? For all of this, we we have two reads to set the tone. Make sure to get those in first thing before moving on to the next bits.\n\nFirst half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.\nThe 2018 Atlantic piece “The scientific paper is obsolete” on computational notebooks, by James Somers.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "intro.html#modern-scientific-tools",
    "href": "intro.html#modern-scientific-tools",
    "title": "1 Introduction",
    "section": "Modern Scientific Tools",
    "text": "Modern Scientific Tools\nOnce we know a bit more about why we should care about the tools we use, let’s dig into those that will underpin much of this course. This part is interesting in itself, but will also valuable to better understand the practical aspects of the course. Again, we have two reads here to set the tone and complement the practical introduction we saw in the Hands-on and DIY parts of the previous block. We are closing the circle here:\n\nSecond half of Chapter 1 in “Geographic Data Science with Python” Geographic Thinking for Data Scientists.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "openscienceR.html",
    "href": "openscienceR.html",
    "title": "OpenScience in R",
    "section": "",
    "text": "Data wrangling\nNow that you know what computational notebooks are and why we should care about them, let’s start using them! This section introduces you to using R for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running the code on your own.\nOnce you have read through, jump on the Do-It-Yourself section, which will provide you with a challenge that you should complete on your own, and will allow you to put what you have already learnt into practice.\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in R so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#loading-packages",
    "href": "openscienceR.html#loading-packages",
    "title": "OpenScience in R",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\nlibrary(tidyverse) # a structure of data manipulation including several packages \nlibrary(data.table) # load the data.table package",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#datasets",
    "href": "openscienceR.html#datasets",
    "title": "OpenScience in R",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 &lt;- read.csv(\"data/census2021_ethn/liv_pop.csv\", row.names = \"GeographyCode\")\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read.csv from base R, you could also use read_csv from library(\"readr\").\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument row.names is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read.csv because the file we want to read is in the csv format. However, many more formats can be read into an R environment. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read.csv.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the census2021_ethn file by right-clicking on this link and saving the file\nPlace the file in a data folder you have created where you intend to read it.\nYour folder should have the following structure a. a gds folder (where you will save your quarto .qmd documents) b. a data folder c. the census2021_ethn folder inside your data folder.\n\n\n\n\n\n\n\n\n\nDownload a folder on github\n\n\n\n\nFirst go to https://download-directory.github.io/\nThen go to the folder you need to today. So for example copy: https://github.com/pietrostefani/gds/tree/main/data/London\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#data-sliced-and-diced",
    "href": "openscienceR.html#data-sliced-and-diced",
    "title": "OpenScience in R",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\nview(census2021)\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, tidyverse in R will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\nhead(census2021) # read first 5 rows\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania\nE01006512                      0\nE01006513                      7\nE01006514                      5\nE01006515                      2\nE01006518                      4\nE01006519                      3\n\ntail(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033763   1302     68                  142                             11\nE01033764   2106     32                   49                             15\nE01033765   1277     21                   33                             17\nE01033766   1028     12                   20                              8\nE01033767   1003     29                   29                              5\nE01033768   1016     69                  111                             21\n          Antarctica.and.Oceania\nE01033763                      4\nE01033764                      0\nE01033765                      3\nE01033766                      7\nE01033767                      1\nE01033768                      6\n\n\nSummarise\nWe can get an overview of the values of the table:\n\nsummary(census2021)\n\n     Europe         Africa       Middle.East.and.Asia\n Min.   : 731   Min.   :  0.00   Min.   :  1.00      \n 1st Qu.:1331   1st Qu.:  7.00   1st Qu.: 16.00      \n Median :1446   Median : 14.00   Median : 33.50      \n Mean   :1462   Mean   : 29.82   Mean   : 62.91      \n 3rd Qu.:1580   3rd Qu.: 30.00   3rd Qu.: 62.75      \n Max.   :2551   Max.   :484.00   Max.   :840.00      \n The.Americas.and.the.Caribbean Antarctica.and.Oceania\n Min.   : 0.000                 Min.   : 0.00         \n 1st Qu.: 2.000                 1st Qu.: 0.00         \n Median : 5.000                 Median : 1.00         \n Mean   : 8.087                 Mean   : 1.95         \n 3rd Qu.:10.000                 3rd Qu.: 3.00         \n Max.   :61.000                 Max.   :11.00         \n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\nt(summary(census2021))\n\n                                                                \n    Europe                     Min.   : 731     1st Qu.:1331    \n    Africa                     Min.   :  0.00   1st Qu.:  7.00  \nMiddle.East.and.Asia           Min.   :  1.00   1st Qu.: 16.00  \nThe.Americas.and.the.Caribbean Min.   : 0.000   1st Qu.: 2.000  \nAntarctica.and.Oceania         Min.   : 0.00    1st Qu.: 0.00   \n                                                                \n    Europe                     Median :1446     Mean   :1462    \n    Africa                     Median : 14.00   Mean   : 29.82  \nMiddle.East.and.Asia           Median : 33.50   Mean   : 62.91  \nThe.Americas.and.the.Caribbean Median : 5.000   Mean   : 8.087  \nAntarctica.and.Oceania         Median : 1.00    Mean   : 1.95   \n                                                                \n    Europe                     3rd Qu.:1580     Max.   :2551    \n    Africa                     3rd Qu.: 30.00   Max.   :484.00  \nMiddle.East.and.Asia           3rd Qu.: 62.75   Max.   :840.00  \nThe.Americas.and.the.Caribbean 3rd Qu.:10.000   Max.   :61.000  \nAntarctica.and.Oceania         3rd Qu.: 3.00    Max.   :11.00",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#columns",
    "href": "openscienceR.html#columns",
    "title": "OpenScience in R",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nUsing the package dplyr. More info can be found here. dplyr is a part of the tidyverse!\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(Total_Pop = rowSums(select(., Africa, Middle.East.and.Asia, Europe, The.Americas.and.the.Caribbean, Antarctica.and.Oceania)))\n\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Pop\nE01006512                      0      1880\nE01006513                      7      2941\nE01006514                      5      2108\nE01006515                      2      1208\nE01006518                      4      1696\nE01006519                      3      1286\n\n\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\ncensus2021$new_column &lt;- 1\nhead(census2021)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006512    910    106                  840                             24\nE01006513   2225     61                  595                             53\nE01006514   1786     63                  193                             61\nE01006515    974     29                  185                             18\nE01006518   1531     69                   73                             19\nE01006519   1238      7                   24                             14\n          Antarctica.and.Oceania Total_Pop new_column\nE01006512                      0      1880          1\nE01006513                      7      2941          1\nE01006514                      5      2108          1\nE01006515                      2      1208          1\nE01006518                      4      1696          1\nE01006519                      3      1286          1\n\n\ndplyr is an immensely useful package in R because it streamlines and simplifies the process of data manipulation and transformation. With its intuitive and consistent syntax, dplyr provides a set of powerful and efficient functions that make tasks like filtering, summarizing, grouping, and joining datasets much more straightforward. Whether you’re working with small or large datasets, dplyr’s optimized code execution ensures fast and efficient operations. Its ability to chain functions together using the pipe operator (%&gt;%) allows for a clean and readable code structure, enhancing code reproducibility and collaboration. Overall, dplyr is an indispensable tool for data analysts and scientists working in R, enabling them to focus on their data insights rather than wrestling with complex data manipulation code.\nDelete columns\nPermanently deleting variables is also within reach of one command:\ndplyr\n\ncensus2021 &lt;- census2021 %&gt;%\n  mutate(new_column = 1)\n\n\ncensus2021 &lt;- census2021 %&gt;%\n  select(-new_column)",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#queries",
    "href": "openscienceR.html#queries",
    "title": "OpenScience in R",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe can select with c(). If this structure is new to you have a look here.\n\neu_tot_first4 &lt;- census2021 %&gt;%\n  filter(rownames(census2021) %in% c('E01006512', 'E01006513', 'E01006514', 'E01006515')) %&gt;%\n  select(Total_Pop, Europe)\n\neu_tot_first4\n\n          Total_Pop Europe\nE01006512      1880    910\nE01006513      2941   2225\nE01006514      2108   1786\nE01006515      1208    974\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 &lt;- census2021 %&gt;%\n  filter(Total_Pop &gt; 900)\n\nAreas where there are no more than 750 Europeans:\n\neuro750 &lt;- census2021 %&gt;%\n  filter(Europe &lt; 750)\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA &lt;- census2021 %&gt;%\n  filter(`Antarctica.and.Oceania` == 10)\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThese queries can grow in sophistication with almost no limits.\n\n\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 &lt;- census2021 %&gt;%\n  filter(The.Americas.and.the.Caribbean &gt; 25, Total_Pop &lt; 1500)\nac25_l500\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01033750   1235     53                  129                             26\nE01033752   1024     19                  114                             33\nE01033754   1262     37                  112                             32\nE01033756    886     31                  221                             42\nE01033757    731     39                  223                             29\nE01033761   1138     52                  138                             33\n          Antarctica.and.Oceania Total_Pop\nE01033750                      5      1448\nE01033752                      6      1196\nE01033754                      9      1452\nE01033756                      5      1185\nE01033757                      3      1025\nE01033761                     11      1372",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#sorting",
    "href": "openscienceR.html#sorting",
    "title": "OpenScience in R",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted &lt;- census2021 %&gt;%\n  arrange(desc(Total_Pop)) #sorts the dataframe by the \"Total_Pop\" column in descending order \n\nhead(db_pop_sorted)\n\n          Europe Africa Middle.East.and.Asia The.Americas.and.the.Caribbean\nE01006747   2551    163                  812                             24\nE01006513   2225     61                  595                             53\nE01006751   1843    139                  568                             21\nE01006524   2235     36                  125                             24\nE01006787   2187     53                   75                             13\nE01006537   2180     23                   46                              6\n          Antarctica.and.Oceania Total_Pop\nE01006747                      2      3552\nE01006513                      7      2941\nE01006751                      1      2572\nE01006524                     11      2431\nE01006787                      2      2330\nE01006537                      2      2257",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "openscienceR.html#additional-resources",
    "href": "openscienceR.html#additional-resources",
    "title": "OpenScience in R",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in R is the “Data wrangling” chapter in R for Data Science.\nA good extension is Hadley Wickham’ “Tidy data” paper which presents a very popular way of organising tabular data for efficient manipulation.",
    "crumbs": [
      "1 Introduction",
      "OpenScience in R"
    ]
  },
  {
    "objectID": "opensciencePy.html",
    "href": "opensciencePy.html",
    "title": "OpenScience in Python",
    "section": "",
    "text": "Data wrangling\nNow that you know what computational notebooks are and why we should care about them, let’s start using them! This section introduces you to using Python for manipulating tabular data. Please read through it carefully and pay attention to how ideas about manipulating data are translated into code. For this part, you can read directly from the course website, although it is recommended you follow the section interactively by running the code on your own.\nOnce you have read through, jump on the Do-It-Yourself section, which will provide you with a challenge that you should complete on your own, and will allow you to put what you have already learnt into practice.\nReal world datasets tend to be messy. There is no way around it: datasets have “holes” (missing data), the amount of formats in which data can be stored is endless, and the best structure to share data is not always the optimum to analyze them, hence the need to wrangle (manipulate, transform and structure) them. As has been correctly pointed out in many outlets (e.g.), much of the time spent in what is called (Geo-)Data Science is related not only to sophisticated modeling and insight, but to more basic and less exotic tasks such as obtaining data, processing, turning them into a shape that makes analysis possible, and exploring it to get to know their basic properties.\nIn this session, you will use a few real world datasets and learn how to process them in Python so they can be transformed and manipulated, if necessary, and analyzed. For this, we will introduce some of the fundamental tools of data analysis and scientific computing. We use a prepared dataset that saves us much of the more intricate processing that goes beyond the introductory level the session is aimed at.\nIn this notebook, we discuss several patterns to clean and structure data properly, including tidying, subsetting, and aggregating; and we finish with some basic visualization. An additional extension presents more advanced tricks to manipulate tabular data.\nBefore we get our hands data-dirty, let us import all the additional libraries we will need to run the code:",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#loading-packages",
    "href": "opensciencePy.html#loading-packages",
    "title": "OpenScience in Python",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data.\n\n# This ensures visualizations are plotted inside the notebook\n\nimport os              # This provides several system utilities\nimport pandas as pd    # This is the workhorse of data munging in Python",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#datasets",
    "href": "opensciencePy.html#datasets",
    "title": "OpenScience in Python",
    "section": "Datasets",
    "text": "Datasets\nWe will be exploring some demographic characteristics in Liverpool. To do that, we will use a dataset that contains population counts, split by ethnic origin. These counts are aggregated at the Lower Layer Super Output Area (LSOA from now on). LSOAs are an official Census geography defined by the Office of National Statistics. You can think of them, more or less, as neighbourhoods. Many data products (Census, deprivation indices, etc.) use LSOAs as one of their main geographies.\nTo do this, we will download a data folder from github called census2021_ethn. You should place this in a data folder you will use throughout the course.\nImport housesales data from csv\n\ncensus2021 = pd.read_csv(\"data/census2021_ethn/liv_pop.csv\", index_col='GeographyCode')\n\nLet us stop for a minute to learn how we have read the file. Here are the main aspects to keep in mind:\n\nWe are using the method read_csv from the pandas library, which we have imported with the alias pd.\nHere the csv is based on a data file but it could also be a web address or sometimes you find data in packages.\nThe argument index_col is not strictly necessary but allows us to choose one of the columns as the index of the table. More on indices below.\nWe are using read_csv because the file we want to read is in the csv format. However, pandas allows for many more formats to be read and write. A full list of formats supported may be found here.\nTo ensure we can access the data we have read, we store it in an object that we call census2021. We will see more on what we can do with it below but, for now, just keep in mind that allows us to save the result of read_csv.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou need to store the data file on your computer, and read it locally. To do that, you can follow these steps:\n\nDownload the census2021_ethn file by right-clicking on this link and saving the file\nPlace the file in a data folder you have created where you intend to read it.\nYour folder should have the following structure a. a gds folder (where you will save your quarto .qmd documents) b. a data folder c. the census2021_ethn folder inside your data folder.\n\n\n\n\n\n\n\n\n\nDownload a folder on github\n\n\n\n\nFirst go to https://download-directory.github.io/\nThen go to the folder you need to today. So for example copy: https://github.com/pietrostefani/gds/tree/main/data/London\nPaste it in the green box… give it a few minutes\nCheck your downloads file and unzip",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#data-sliced-and-diced",
    "href": "opensciencePy.html#data-sliced-and-diced",
    "title": "OpenScience in Python",
    "section": "Data, sliced and diced",
    "text": "Data, sliced and diced\nNow we are ready to start playing with and interrogating the dataset! What we have at our fingertips is a table that summarizes, for each of the LSOAs in Liverpool, how many people live in each, by the region of the world where they were born. We call these tables DataFrame objects, and they have a lot of functionality built-in to explore and manipulate the data they contain.\nStructure\nLet’s start by exploring the structure of a DataFrame. We can print it by simply typing its name:\n\ncensus2021\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n\n\nE01006513\n2225\n61\n595\n53\n7\n\n\nE01006514\n1786\n63\n193\n61\n5\n\n\nE01006515\n974\n29\n185\n18\n2\n\n\nE01006518\n1531\n69\n73\n19\n4\n\n\n...\n...\n...\n...\n...\n...\n\n\nE01033764\n2106\n32\n49\n15\n0\n\n\nE01033765\n1277\n21\n33\n17\n3\n\n\nE01033766\n1028\n12\n20\n8\n7\n\n\nE01033767\n1003\n29\n29\n5\n1\n\n\nE01033768\n1016\n69\n111\n21\n6\n\n\n\n\n298 rows × 5 columns\n\n\n\nSince they represent a table of data, DataFrame objects have two dimensions: rows and columns. Each of these is automatically assigned a name in what we will call its index. When printing, the index of each dimension is rendered in bold, as opposed to the standard rendering for the content. In the example above, we can see how the column index is automatically picked up from the .csv file’s column names. For rows, we have specified when reading the file we wanted the column GeographyCode, so that is used. If we hadn’t specified any, pandas will automatically generate a sequence starting in 0 and going all the way to the number of rows minus one. This is the standard structure of a DataFrame object, so we will come to it over and over. Importantly, even when we move to spatial data, our datasets will have a similar structure.\nOne further feature of these tables is that they can hold columns with different types of data. In our example, this is not used as we have counts (or int, for integer, types) for each column. But it is useful to keep in mind we can combine this with columns that hold other type of data such as categories, text (str, for string), dates or, as we will see later in the course, geographic features.\nInspecting\nWe can check the top (bottom) X lines of the table by passing X to the method head (tail). For example, for the top/bottom five lines:\n\ncensus2021.head() # read first 5 rows\ncensus2021.tail() # read last 5 rows\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\nE01033764\n2106\n32\n49\n15\n0\n\n\nE01033765\n1277\n21\n33\n17\n3\n\n\nE01033766\n1028\n12\n20\n8\n7\n\n\nE01033767\n1003\n29\n29\n5\n1\n\n\nE01033768\n1016\n69\n111\n21\n6\n\n\n\n\n\n\n\nSummarise\nWe can get an overview of the values of the table:\n\ncensus2021.describe()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\n\n\n\n\ncount\n298.00000\n298.000000\n298.000000\n298.000000\n298.000000\n\n\nmean\n1462.38255\n29.818792\n62.909396\n8.087248\n1.949664\n\n\nstd\n248.67329\n51.606065\n102.519614\n9.397638\n2.168216\n\n\nmin\n731.00000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n25%\n1331.25000\n7.000000\n16.000000\n2.000000\n0.000000\n\n\n50%\n1446.00000\n14.000000\n33.500000\n5.000000\n1.000000\n\n\n75%\n1579.75000\n30.000000\n62.750000\n10.000000\n3.000000\n\n\nmax\n2551.00000\n484.000000\n840.000000\n61.000000\n11.000000\n\n\n\n\n\n\n\nNote how the output is also a DataFrame object, so you can do with it the same things you would with the original table (e.g. writing it to a file).\nIn this case, the summary might be better presented if the table is “transposed”:\n\ncensus2021.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nEurope\n298.0\n1462.382550\n248.673290\n731.0\n1331.25\n1446.0\n1579.75\n2551.0\n\n\nAfrica\n298.0\n29.818792\n51.606065\n0.0\n7.00\n14.0\n30.00\n484.0\n\n\nMiddle East and Asia\n298.0\n62.909396\n102.519614\n1.0\n16.00\n33.5\n62.75\n840.0\n\n\nThe Americas and the Caribbean\n298.0\n8.087248\n9.397638\n0.0\n2.00\n5.0\n10.00\n61.0\n\n\nAntarctica and Oceania\n298.0\n1.949664\n2.168216\n0.0\n0.00\n1.0\n3.00\n11.0",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#columns",
    "href": "opensciencePy.html#columns",
    "title": "OpenScience in Python",
    "section": "Columns",
    "text": "Columns\nCreate new columns\nWe can generate new variables by applying operations on existing ones. For example, we can calculate the total population by area. Here is a couple of ways to do it:\nLonger, hardcoded:\n\ntotal = census2021['Europe'] + census2021['Africa'] + census2021['Middle East and Asia'] + census2021['The Americas and the Caribbean'] + census2021['Antarctica and Oceania']\n# Print the top of the variable\ntotal.head()\n\nGeographyCode\nE01006512    1880\nE01006513    2941\nE01006514    2108\nE01006515    1208\nE01006518    1696\ndtype: int64\n\n\nOne shot:\n\ncensus2021['Total_Population'] = census2021.sum(axis=1)\n# Print the top of the variable\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n\n\n\n\n\n\n\nNote that we are summing over “axis=1”. In a DataFrame object, “axis 0” and “axis 1” represent the rows and columns respectively.\nA different spin on this is assigning new values: we can generate new variables with scalars, and modify those:\n\n# New variable with all ones\ncensus2021['ones'] = 1\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\nones\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n1\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n1\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n1\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n1\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n1\n\n\n\n\n\n\n\nDelete columns\nPermanently deleting variables is also within reach of one command:\n\ndel census2021['ones']\ncensus2021.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#queries",
    "href": "opensciencePy.html#queries",
    "title": "OpenScience in Python",
    "section": "Queries",
    "text": "Queries\nIndex-based queries\nHere we explore how we can subset parts of a DataFrame if we know exactly which bits we want. For example, if we want to extract the total and European population of the first four areas in the table:\nWe use loc with lists:\n\neu_tot_first4 = census2021.loc[['E01006512', 'E01006513', 'E01006514', 'E01006515'], ['Total_Population', 'Europe']]\n\neu_tot_first4\n\n\n\n\n\n\n\n\nTotal_Population\nEurope\n\n\nGeographyCode\n\n\n\n\n\n\nE01006512\n1880\n910\n\n\nE01006513\n2941\n2225\n\n\nE01006514\n2108\n1786\n\n\nE01006515\n1208\n974\n\n\n\n\n\n\n\nCondition-based queries\nHowever, sometimes, we do not know exactly which observations we want, but we do know what conditions they need to satisfy (e.g. areas with more than 2,000 inhabitants). For these cases, DataFrames support selection based on conditions. Let us see a few examples. Suppose we want to select…\nAreas with more than 900 people in Total:\n\npop900 = census2021.loc[census2021['Total_Population'] &gt; 900, :]\npop900\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006512\n910\n106\n840\n24\n0\n1880\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006514\n1786\n63\n193\n61\n5\n2108\n\n\nE01006515\n974\n29\n185\n18\n2\n1208\n\n\nE01006518\n1531\n69\n73\n19\n4\n1696\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nE01033764\n2106\n32\n49\n15\n0\n2202\n\n\nE01033765\n1277\n21\n33\n17\n3\n1351\n\n\nE01033766\n1028\n12\n20\n8\n7\n1075\n\n\nE01033767\n1003\n29\n29\n5\n1\n1067\n\n\nE01033768\n1016\n69\n111\n21\n6\n1223\n\n\n\n\n298 rows × 6 columns\n\n\n\nAreas where there are no more than 750 Europeans:\n\neuro750 = census2021.loc[census2021['Europe'] &lt; 750, :]\neuro750\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01033757\n731\n39\n223\n29\n3\n1025\n\n\n\n\n\n\n\nAreas with exactly ten person from Antarctica and Oceania:\n\noneOA = census2021.loc[census2021['Antarctica and Oceania'] == 10, :]\noneOA\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006679\n1353\n484\n354\n31\n10\n2232\n\n\n\n\n\n\n\nPro-tip: These queries can grow in sophistication with almost no limits.\nCombining queries\nNow all of these queries can be combined with each other, for further flexibility. For example, imagine we want areas with more than 25 people from the Americas and Caribbean, but less than 1,500 in total:\n\nac25_l500 = census2021.loc[(census2021['The Americas and the Caribbean'] &gt; 25) &                    (census2021['Total_Population'] &lt; 1500), :]\nac25_l500\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01033750\n1235\n53\n129\n26\n5\n1448\n\n\nE01033752\n1024\n19\n114\n33\n6\n1196\n\n\nE01033754\n1262\n37\n112\n32\n9\n1452\n\n\nE01033756\n886\n31\n221\n42\n5\n1185\n\n\nE01033757\n731\n39\n223\n29\n3\n1025\n\n\nE01033761\n1138\n52\n138\n33\n11\n1372",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#sorting",
    "href": "opensciencePy.html#sorting",
    "title": "OpenScience in Python",
    "section": "Sorting",
    "text": "Sorting\nAmong the many operations DataFrame objects support, one of the most useful ones is to sort a table based on a given column. For example, imagine we want to sort the table by total population:\n\ndb_pop_sorted = census2021.sort_values('Total_Population', ascending=False)\ndb_pop_sorted.head()\n\n\n\n\n\n\n\n\nEurope\nAfrica\nMiddle East and Asia\nThe Americas and the Caribbean\nAntarctica and Oceania\nTotal_Population\n\n\nGeographyCode\n\n\n\n\n\n\n\n\n\n\nE01006747\n2551\n163\n812\n24\n2\n3552\n\n\nE01006513\n2225\n61\n595\n53\n7\n2941\n\n\nE01006751\n1843\n139\n568\n21\n1\n2572\n\n\nE01006524\n2235\n36\n125\n24\n11\n2431\n\n\nE01006787\n2187\n53\n75\n13\n2\n2330",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  },
  {
    "objectID": "opensciencePy.html#additional-resources",
    "href": "opensciencePy.html#additional-resources",
    "title": "OpenScience in Python",
    "section": "Additional resources",
    "text": "Additional resources\n\nA good introduction to data manipulation in Python is Wes McKinney’s “Python for Data Analysis”",
    "crumbs": [
      "1 Introduction",
      "OpenScience in Python"
    ]
  }
]